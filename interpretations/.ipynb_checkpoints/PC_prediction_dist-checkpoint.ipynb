{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, paired_distances\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "base_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(base_dir)\n",
    "sys.path.append(f\"{base_dir}/turing/examples-raw/gluesst_finetune/\")\n",
    "sys.path.append(f\"{base_dir}/turing/src/\")\n",
    "\n",
    "from argparse import Namespace\n",
    "from methods.bag_of_ngrams.processing import cleanReports, cleanSplit, stripChars\n",
    "from methods.interpretations.utils import compute_input_type_attention\n",
    "from methods.interpretations.integrated_gradients.utils import forward_with_softmax, summarize_attributions\n",
    "from pyfunctions.general import extractListFromDic, readJson\n",
    "from pyfunctions.pathology import extract_synoptic, fixLabelProstateGleason, fixProstateLabels, fixLabel, exclude_labels\n",
    "from pyfunctions.feature_anaysis_utils import center, calculate_geometry, compute_RSA, get_projection, low_rank_approx, rank_1_approx, get_acc\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from turing.pathology.path_utils import evaluate, extract_features, load_tnlr_base, load_tnlr_tokenizer, path_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da5f7ae",
   "metadata": {},
   "source": [
    "# PC prediction Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c04ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['bert', 'tnlr', 'pubmed_bert', 'biobert', 'clinical_biobert']\n",
    "features = []\n",
    "field = 'SeminalVesicleNone' #'PrimaryGleason','SecondaryGleason', 'MarginStatusNone', 'SeminalVesicleNone'\n",
    "\n",
    "for m in models:\n",
    "    model_folder = f\"{base_dir}/output/rsa/{m}/{field}\"\n",
    "\n",
    "    p = os.path.join(model_folder, f\"1000_cls_logits_l12_ft_best.pkl\")\n",
    "    # open pkl file\n",
    "    with open(p, 'rb') as handle:\n",
    "        f = pickle.load(handle)\n",
    "        features.append(f)\n",
    "center_f = center(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model_type': 'clinical_biobert', # tnlr, bert, pubmed_bert, biobert, clinical_biobert\n",
    "    'task_name': 'sst-2',\n",
    "    'do_train': False,\n",
    "    'do_eval': True,\n",
    "    'evaluate_during_training': True,\n",
    "    'max_seq_length': 512,\n",
    "    'do_lower_case': True,\n",
    "    'per_gpu_train_batch_size': 8,\n",
    "    'per_gpu_eval_batch_size': 8,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'learning_rate': 7e-6,\n",
    "    'weight_decay': 0.0,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_grad_norm': 1,\n",
    "    'num_train_epochs': 3.0,\n",
    "    'max_steps': -1,\n",
    "    'warmup_ratio': 0.2,\n",
    "    'logging_steps': 50,\n",
    "    'eval_all_checkpoints': True,\n",
    "    'no_cuda': False,\n",
    "    'seed': 42,\n",
    "    'metric_for_choose_best_checkpoint': None,\n",
    "    'fp16': False,\n",
    "    'fp16_opt_level': 'O1',\n",
    "    'local_rank': -1,\n",
    "    'num_train_epochs': 25, \n",
    "    'n_gpu': 1,\n",
    "    'device': 'cuda',\n",
    "    'run': 0\n",
    "}\n",
    "\n",
    "kwargs = Namespace(**args)\n",
    "\n",
    "if args['model_type'] == 'bert':\n",
    "    bert_path = 'bert-base-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "elif args['model_type'] == 'pubmed_bert':\n",
    "    bert_path = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "elif args['model_type'] == 'biobert':\n",
    "    bert_path = \"dmis-lab/biobert-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "elif args['model_type'] == 'clinical_biobert':\n",
    "    bert_path = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "elif args['model_type'] == 'tnlr':\n",
    "    vocab_file = f'{base_dir}/turing/src/tnlr/tokenizer/tnlr-uncased-vocab.txt'\n",
    "    tokenizer = load_tnlr_tokenizer(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "path = f\"../data/prostate.json\"\n",
    "data = readJson(path)\n",
    "\n",
    "# Clean reports\n",
    "data = cleanSplit(data, stripChars)\n",
    "data['dev_test'] = cleanReports(data['dev_test'], stripChars)\n",
    "data = fixLabel(data)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['train']]\n",
    "train_labels = [patient['labels'][field] for patient in data['train']]\n",
    "\n",
    "val_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['val']]\n",
    "val_labels = [patient['labels'][field] for patient in data['val']]\n",
    "\n",
    "test_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['test']]\n",
    "test_labels = [patient['labels'][field] for patient in data['test']]\n",
    "\n",
    "# Exclude '2' and 'null'\n",
    "if field in ['PrimaryGleason', 'SecondaryGleason']:\n",
    "    train_documents, train_labels = exclude_labels(train_documents, train_labels)\n",
    "    val_documents, val_labels = exclude_labels(val_documents, val_labels)\n",
    "    test_documents, test_labels = exclude_labels(test_documents, test_labels)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "\n",
    "# Handle new class\n",
    "le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "le_dict = {str(key):le_dict[key] for key in le_dict}\n",
    "\n",
    "for label in val_labels + test_labels:\n",
    "    if str(label) not in le_dict:\n",
    "        le_dict[str(label)] = len(le_dict)\n",
    "\n",
    "# Map processed label back to raw label\n",
    "inv_le_dict = {v: k for k, v in le_dict.items()}\n",
    "\n",
    "documents_full = train_documents + val_documents + test_documents\n",
    "labels_full = train_labels + val_labels + test_labels\n",
    "\n",
    "p_test = len(test_labels)/len(labels_full)\n",
    "p_val = len(val_labels)/(len(train_labels) + len(val_labels))\n",
    "\n",
    "train_docs, test_docs, train_labels, test_labels = train_test_split(documents_full, \n",
    "                                                                    labels_full, \n",
    "                                                                    test_size= p_test,\n",
    "                                                                    random_state=args['run'])\n",
    "\n",
    "train_docs, val_docs, train_labels, val_labels = train_test_split(train_docs, \n",
    "                                                                  train_labels, \n",
    "                                                                  test_size= p_val,\n",
    "                                                                  random_state=args['run'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{base_dir}/output/fine_tuning/{args['model_type']}_{0}/{field}\"\n",
    "checkpoint_file = f\"{model_path}/save_output\"\n",
    "config_file = f\"{model_path}/save_output/config.json\"\n",
    "\n",
    "if args['model_type'] != 'tnlr':\n",
    "    model = BertForSequenceClassification.from_pretrained(checkpoint_file, num_labels=len(le_dict), output_hidden_states=True)\n",
    "else:\n",
    "    model = load_tnlr_base(checkpoint_file, config_file, model_type='tnlrv3_classification', num_labels=len(le_dict))\n",
    "    model.config.update({'output_hidden_states': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9980f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank1_result = {m:{'f1':[], 'per_class':[], 'dist':[]} for m in models} # comment this line out to gather results for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10cac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank-1 approx\n",
    "true_labels = train_labels[:1000]\n",
    "device = args['device']\n",
    "inv_model_dict = {m:i for i, m in enumerate(models)}\n",
    "\n",
    "cf = center_f[inv_model_dict[args['model_type']]]\n",
    "\n",
    "for k in [0, 1]:\n",
    "    W_k = rank_1_approx(k, cf)\n",
    "    \n",
    "    with torch.cuda.device(1):\n",
    "        model = model.eval()\n",
    "        model.to(device)\n",
    "        W_k = torch.from_numpy(W_k[:, np.newaxis, :]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a1 = model.bert.pooler(W_k)\n",
    "            a2 = model.dropout(a1)\n",
    "            logits = model.classifier(a2)\n",
    "\n",
    "        f1, per_class, dist = get_acc(logits, true_labels)\n",
    "    \n",
    "    rank1_result[args['model_type']]['f1'].append(f1)\n",
    "    rank1_result[args['model_type']]['per_class'].append(per_class)\n",
    "    rank1_result[args['model_type']]['dist'].append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f867f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after gathering results for all models:\n",
    "rank1_result['True'] = {}\n",
    "true_label_ids = np.array([le_dict[l] for l in true_labels])\n",
    "rank1_result['True']['dist'] = [collections.Counter(true_label_ids), collections.Counter(true_label_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf19a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = {}\n",
    "for m in rank1_result.keys():\n",
    "    results_dt = pd.DataFrame(columns=['Direction', 'Count', 'Class'])\n",
    "    i = 0\n",
    "    pc1_dist = rank1_result[m]['dist'][0]\n",
    "    for k, c in pc1_dist.items():\n",
    "        str_class = inv_le_dict[k]\n",
    "        row = pd.Series({'Direction': 'PC1', 'Class': str_class, 'Count': c}, name=i)\n",
    "        results_dt = results_dt.append(row)\n",
    "        i+=1\n",
    "    dt1[m] = results_dt\n",
    "\n",
    "dt2 = {}\n",
    "for m in rank1_result.keys():\n",
    "    results_dt = pd.DataFrame(columns=['Direction', 'Count', 'Class'])\n",
    "    i = 0\n",
    "    pc2_dist = rank1_result[m]['dist'][1]\n",
    "    for k, c in pc2_dist.items():\n",
    "        str_class = inv_le_dict[k]\n",
    "        row = pd.Series({'Direction': 'PC2', 'Class': str_class, 'Count': c}, name=i)\n",
    "        results_dt = results_dt.append(row)\n",
    "        i+=1\n",
    "    dt2[m] = results_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900591c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1['bert']['model'] = 'BERT'\n",
    "dt1['tnlr']['model'] = 'TNLR'\n",
    "dt1['pubmed_bert']['model'] = 'PubMed BERT'\n",
    "dt1['biobert']['model'] = 'BioBERT'\n",
    "dt1['clinical_biobert']['model'] = 'Clinical BioBERT'\n",
    "dt1['True']['model'] = 'True'\n",
    "\n",
    "dt2['bert']['model'] = 'BERT'\n",
    "dt2['tnlr']['model'] = 'TNLR'\n",
    "dt2['pubmed_bert']['model'] = 'PubMed BERT'\n",
    "dt2['biobert']['model'] = 'BioBERT'\n",
    "dt2['clinical_biobert']['model'] = 'Clinical BioBERT'\n",
    "dt2['True']['model'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 'PrimaryGleason' and 'SecondaryGleason'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac049023",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([dt1['True'], dt1['bert'], dt1['tnlr'], dt1['pubmed_bert'], dt1['biobert'], dt1['clinical_biobert'],\n",
    "                  dt2['True'], dt2['bert'], dt2['tnlr'], dt2['pubmed_bert'], dt2['biobert'], dt2['clinical_biobert']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(kind='bar', data=data, col='Direction', x='model', y='Count', hue='Class',\n",
    "                hue_order = ['3', '4', '5'], palette=sns.color_palette(\"Set2\"))\n",
    "g.set_xticklabels(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fdd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 'MarginStatusNone' and 'SeminalVesicleNone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3013da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([dt1['True'], dt1['bert'], dt1['tnlr'], dt1['pubmed_bert'], dt1['biobert'], dt1['clinical_biobert']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5247225",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(kind='bar', data=data, col='Direction', x='model', y='Count', hue='Class',\n",
    "                hue_order = ['0', '1'], palette=sns.color_palette(\"Set2\"))\n",
    "g.set_xticklabels(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8cfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
