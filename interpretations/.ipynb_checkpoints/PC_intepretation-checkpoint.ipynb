{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0278ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, paired_distances\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "base_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(base_dir)\n",
    "sys.path.append(f\"{base_dir}/turing/examples-raw/gluesst_finetune/\")\n",
    "sys.path.append(f\"{base_dir}/turing/src/\")\n",
    "\n",
    "from argparse import Namespace\n",
    "from methods.bag_of_ngrams.processing import cleanReports, cleanSplit, stripChars\n",
    "from methods.interpretations.utils import compute_input_type_attention\n",
    "from methods.interpretations.integrated_gradients.utils import forward_with_softmax, summarize_attributions\n",
    "from pyfunctions.general import extractListFromDic, readJson\n",
    "from pyfunctions.pathology import extract_synoptic, fixLabelProstateGleason, fixProstateLabels, fixLabel, exclude_labels\n",
    "from pyfunctions.feature_anaysis_utils import center, calculate_geometry, compute_RSA, get_projection, low_rank_approx, rank_1_approx, get_acc\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from turing.pathology.path_utils import evaluate, extract_features, load_tnlr_base, load_tnlr_tokenizer, path_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1a0ae",
   "metadata": {},
   "source": [
    "# Finetuned feature space is highly sparsified -- PC explained ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf0c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['bert', 'tnlr', 'pubmed_bert', 'biobert', 'clinical_biobert']\n",
    "features = []\n",
    "field = 'PrimaryGleason' #'PrimaryGleason','SecondaryGleason', 'MarginStatusNone', 'SeminalVesicleNone'\n",
    "\n",
    "for m in models:\n",
    "    model_folder = f\"{base_dir}/output/rsa/{m}/{field}\"\n",
    "\n",
    "    p = os.path.join(model_folder, f\"1000_cls_logits_l12_ft_best.pkl\")\n",
    "    # open pkl file\n",
    "    with open(p, 'rb') as handle:\n",
    "        f = pickle.load(handle)\n",
    "        features.append(f)\n",
    "center_f = center(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e88805",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_exp_ratios = {m:[] for m in models}\n",
    "\n",
    "for i, cf in enumerate(center_f):\n",
    "    my_model = PCA(n_components=5)\n",
    "    my_model.fit_transform(cf)\n",
    "    pc_exp_ratios[models[i]].extend(my_model.explained_variance_ratio_.cumsum())\n",
    "    #print (my_model.explained_variance_)\n",
    "    #print (my_model.explained_variance_ratio_)\n",
    "    #print (my_model.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae956e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = {}\n",
    "for m in models:\n",
    "    results_dt = pd.DataFrame(columns=['PC Index', 'Explained Variance Ratio - Cumulative Sum'])\n",
    "    i = 0\n",
    "    for l in range(5):\n",
    "        row = pd.Series({'PC Index': l, 'Explained Variance Ratio - Cumulative Sum': pc_exp_ratios[m][l]}, name=i)\n",
    "        results_dt = results_dt.append(row)\n",
    "        i+=1\n",
    "    dt[m] = results_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dd44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['bert']['model'] = 'BERT'\n",
    "dt['tnlr']['model'] = 'TNLR'\n",
    "dt['pubmed_bert']['model'] = 'PubMed BERT'\n",
    "dt['biobert']['model'] = 'BioBERT'\n",
    "dt['clinical_biobert']['model'] = 'Clinical BioBERT'\n",
    "\n",
    "data = pd.concat([dt['bert'], dt['tnlr'], dt['pubmed_bert'], dt['biobert'], dt['clinical_biobert']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a2002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(os.path.join(f\"{base_dir}/theme_bw.mplstyle\"))\n",
    "\n",
    "plt.figure(figsize=(10, 7.5))\n",
    "\n",
    "# Remove the plot frame lines. They are unnecessary chartjunk.  \n",
    "ax = plt.subplot(111)  \n",
    "ax.spines[\"top\"].set_visible(False)   \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "\n",
    "# Ensure that the axis ticks only show up on the bottom and left of the plot.  \n",
    "# Ticks on the right and top of the plot are generally unnecessary chartjunk.  \n",
    "ax.get_xaxis().tick_bottom()  \n",
    "ax.get_yaxis().tick_left()\n",
    "\n",
    "ax.set_xticks([0, 1, 2, 3, 4]) \n",
    "\n",
    "plt.xlabel('PC Index') \n",
    "plt.ylabel('Cumulative Sum') \n",
    "\n",
    "plt.ylim((0.6, 1))\n",
    "\n",
    "sns.lineplot(x = 'PC Index', y = 'Explained Variance Ratio - Cumulative Sum', data=data, hue='model', marker='o', ci=None)#, palette=color_scheme)\n",
    "\n",
    "plt.title(\"PC Explained Variance Ratio - Primary Gleason\")\n",
    "plt.legend(title='Model', labels=['BERT', 'TNLR','PubMEd BERT', 'BioBERT','Clinical BioBERT'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig('micro.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278e612",
   "metadata": {},
   "source": [
    "# Accuracy probing: remove principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer-wise analysis\n",
    "models = ['bert', 'tnlr', 'pubmed_bert', 'biobert', 'clinical_biobert']\n",
    "features = []\n",
    "field = 'SeminalVesicleNone' #'PrimaryGleason','SecondaryGleason', 'MarginStatusNone', 'SeminalVesicleNone'\n",
    "\n",
    "for m in models:\n",
    "    model_folder = f\"{base_dir}/output/rsa/{m}/{field}\"\n",
    "\n",
    "    p = os.path.join(model_folder, f\"1000_cls_logits_l12_ft_best.pkl\")\n",
    "    # open pkl file\n",
    "    with open(p, 'rb') as handle:\n",
    "        f = pickle.load(handle)\n",
    "        features.append(f)\n",
    "center_f = center(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model params\n",
    "args = {\n",
    "    'model_type': 'clinical_biobert', # tnlr, bert, pubmed_bert, biobert, clinical_biobert\n",
    "    'task_name': 'sst-2',\n",
    "    'do_train': False,\n",
    "    'do_eval': True,\n",
    "    'evaluate_during_training': True,\n",
    "    'max_seq_length': 512,\n",
    "    'do_lower_case': True,\n",
    "    'per_gpu_train_batch_size': 8,\n",
    "    'per_gpu_eval_batch_size': 8,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'learning_rate': 7e-6,\n",
    "    'weight_decay': 0.0,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_grad_norm': 1,\n",
    "    'num_train_epochs': 3.0,\n",
    "    'max_steps': -1,\n",
    "    'warmup_ratio': 0.2,\n",
    "    'logging_steps': 50,\n",
    "    'eval_all_checkpoints': True,\n",
    "    'no_cuda': False,\n",
    "    'seed': 42,\n",
    "    'metric_for_choose_best_checkpoint': None,\n",
    "    'fp16': False,\n",
    "    'fp16_opt_level': 'O1',\n",
    "    'local_rank': -1,\n",
    "    'num_train_epochs': 25, \n",
    "    'n_gpu': 1,\n",
    "    'device': 'cuda',\n",
    "    'run': 0\n",
    "}\n",
    "\n",
    "kwargs = Namespace(**args)\n",
    "\n",
    "if args['model_type'] == 'bert':\n",
    "    bert_path = 'bert-base-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "elif args['model_type'] == 'pubmed_bert':\n",
    "    bert_path = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "elif args['model_type'] == 'biobert':\n",
    "    bert_path = \"dmis-lab/biobert-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "elif args['model_type'] == 'clinical_biobert':\n",
    "    bert_path = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "elif args['model_type'] == 'tnlr':\n",
    "    vocab_file = f'{base_dir}/turing/src/tnlr/tokenizer/tnlr-uncased-vocab.txt'\n",
    "    tokenizer = load_tnlr_tokenizer(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "path = f\"../data/prostate.json\"\n",
    "data = readJson(path)\n",
    "\n",
    "# Clean reports\n",
    "data = cleanSplit(data, stripChars)\n",
    "data['dev_test'] = cleanReports(data['dev_test'], stripChars)\n",
    "data = fixLabel(data)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['train']]\n",
    "train_labels = [patient['labels'][field] for patient in data['train']]\n",
    "\n",
    "val_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['val']]\n",
    "val_labels = [patient['labels'][field] for patient in data['val']]\n",
    "\n",
    "test_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['test']]\n",
    "test_labels = [patient['labels'][field] for patient in data['test']]\n",
    "\n",
    "# Exclude '2' and 'null'\n",
    "if field in ['PrimaryGleason', 'SecondaryGleason']:\n",
    "    train_documents, train_labels = exclude_labels(train_documents, train_labels)\n",
    "    val_documents, val_labels = exclude_labels(val_documents, val_labels)\n",
    "    test_documents, test_labels = exclude_labels(test_documents, test_labels)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "\n",
    "# Handle new class\n",
    "le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "le_dict = {str(key):le_dict[key] for key in le_dict}\n",
    "\n",
    "for label in val_labels + test_labels:\n",
    "    if str(label) not in le_dict:\n",
    "        le_dict[str(label)] = len(le_dict)\n",
    "\n",
    "# Map processed label back to raw label\n",
    "inv_le_dict = {v: k for k, v in le_dict.items()}\n",
    "\n",
    "documents_full = train_documents + val_documents + test_documents\n",
    "labels_full = train_labels + val_labels + test_labels\n",
    "\n",
    "p_test = len(test_labels)/len(labels_full)\n",
    "p_val = len(val_labels)/(len(train_labels) + len(val_labels))\n",
    "\n",
    "train_docs, test_docs, train_labels, test_labels = train_test_split(documents_full, \n",
    "                                                                    labels_full, \n",
    "                                                                    test_size= p_test,\n",
    "                                                                    random_state=args['run'])\n",
    "\n",
    "train_docs, val_docs, train_labels, val_labels = train_test_split(train_docs, \n",
    "                                                                  train_labels, \n",
    "                                                                  test_size= p_val,\n",
    "                                                                  random_state=args['run'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e22c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model\n",
    "model_path = f\"{base_dir}/output/fine_tuning/{args['model_type']}_{0}/{field}\"\n",
    "checkpoint_file = f\"{model_path}/save_output\"\n",
    "config_file = f\"{model_path}/save_output/config.json\"\n",
    "\n",
    "if args['model_type'] != 'tnlr':\n",
    "    model = BertForSequenceClassification.from_pretrained(checkpoint_file, num_labels=len(le_dict), output_hidden_states=True)\n",
    "else:\n",
    "    model = load_tnlr_base(checkpoint_file, config_file, model_type='tnlrv3_classification', num_labels=len(le_dict))\n",
    "    model.config.update({'output_hidden_states': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_rm_result = {m:{'f1':[]} for m in models} # comment this line out to gather results for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = train_labels[:1000]\n",
    "\n",
    "# f1 vs pc removal\n",
    "device = args['device']\n",
    "inv_model_dict = {m:i for i, m in enumerate(models)}\n",
    "\n",
    "cf = center_f[inv_model_dict[args['model_type']]]\n",
    "d = cf.shape[1]\n",
    "\n",
    "for k in tqdm(range(d-1, -1, -1)):\n",
    "    W_k = low_rank_approx(k, cf)\n",
    "    \n",
    "    with torch.cuda.device(1):\n",
    "        model = model.eval()\n",
    "        model.to(device)\n",
    "        W_k = torch.from_numpy(W_k[:, np.newaxis, :]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a1 = model.bert.pooler(W_k)\n",
    "            a2 = model.dropout(a1)\n",
    "            logits = model.classifier(a2)\n",
    "\n",
    "        f1, _, _ = get_acc(logits, true_labels)\n",
    "    \n",
    "    pc_rm_result[args['model_type']]['f1'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90939d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after gathering results for all models:\n",
    "dt = {}\n",
    "for m in models:\n",
    "    results_dt = pd.DataFrame(columns=['PC Index', 'F1'])\n",
    "    i = 0\n",
    "    for l in range(d):\n",
    "        row = pd.Series({'PC Index': l, 'F1': pc_rm_result[m]['f1'][l]}, name=i)\n",
    "        results_dt = results_dt.append(row)\n",
    "        i+=1\n",
    "    dt[m] = results_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d2e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['bert']['model'] = 'BERT'\n",
    "dt['tnlr']['model'] = 'TNLR'\n",
    "dt['pubmed_bert']['model'] = 'PubMed BERT'\n",
    "dt['biobert']['model'] = 'BioBERT'\n",
    "dt['clinical_biobert']['model'] = 'Clinical BioBERT'\n",
    "\n",
    "data = pd.concat([dt['bert'], dt['tnlr'], dt['pubmed_bert'], dt['biobert'], dt['clinical_biobert']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(os.path.join(f\"{base_dir}/theme_bw.mplstyle\"))\n",
    "\n",
    "plt.figure(figsize=(10, 7.5))\n",
    "\n",
    "# Remove the plot frame lines. They are unnecessary chartjunk.  \n",
    "ax = plt.subplot(111)  \n",
    "ax.spines[\"top\"].set_visible(False)   \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "\n",
    "# Ensure that the axis ticks only show up on the bottom and left of the plot.  \n",
    "# Ticks on the right and top of the plot are generally unnecessary chartjunk.  \n",
    "ax.get_xaxis().tick_bottom()  \n",
    "ax.get_yaxis().tick_left()\n",
    "\n",
    "#ax.set_xticks(range(769)) \n",
    "\n",
    "plt.xlabel('First Bottom k PC(s) Used') \n",
    "plt.ylabel('F1') \n",
    "\n",
    "#plt.xlim((0, 769))\n",
    "\n",
    "sns.lineplot(x = 'PC Index', y = 'F1', data=data, hue='model', ci=None)#, palette=color_scheme)\n",
    "\n",
    "plt.title(\"PC Probing - Primary Gleason\")\n",
    "plt.legend(title='Model', labels=['BERT', 'TNLR','PubMEd BERT', 'BioBERT','Clinical BioBERT'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig('micro.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183ee88",
   "metadata": {},
   "source": [
    "## close-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed34411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = {}\n",
    "for m in models:\n",
    "    results_dt = pd.DataFrame(columns=['PC Index', 'F1'])\n",
    "    i = 0\n",
    "    for l in range(720, d):\n",
    "        row = pd.Series({'PC Index': l, 'F1': pc_rm_result[m]['f1'][l]}, name=i)\n",
    "        results_dt = results_dt.append(row)\n",
    "        i+=1\n",
    "    dt[m] = results_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['bert']['model'] = 'BERT'\n",
    "dt['tnlr']['model'] = 'TNLR'\n",
    "dt['pubmed_bert']['model'] = 'PubMed BERT'\n",
    "dt['biobert']['model'] = 'BioBERT'\n",
    "dt['clinical_biobert']['model'] = 'Clinical BioBERT'\n",
    "\n",
    "data = pd.concat([dt['bert'], dt['tnlr'], dt['pubmed_bert'], dt['biobert'], dt['clinical_biobert']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef2f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(os.path.join(f\"{base_dir}/theme_bw.mplstyle\"))\n",
    "\n",
    "plt.figure(figsize=(10, 7.5))\n",
    "\n",
    "# Remove the plot frame lines. They are unnecessary chartjunk.  \n",
    "ax = plt.subplot(111)  \n",
    "ax.spines[\"top\"].set_visible(False)   \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "\n",
    "# Ensure that the axis ticks only show up on the bottom and left of the plot.  \n",
    "# Ticks on the right and top of the plot are generally unnecessary chartjunk.  \n",
    "ax.get_xaxis().tick_bottom()  \n",
    "ax.get_yaxis().tick_left()\n",
    "\n",
    "#ax.set_xticks(range(769)) \n",
    "\n",
    "plt.xlabel('First Bottom k PC(s) Used') \n",
    "plt.ylabel('F1') \n",
    "\n",
    "plt.xlim((719, 770))\n",
    "\n",
    "sns.lineplot(x = 'PC Index', y = 'F1', data=data, hue='model', marker='o', ci=None)#, palette=color_scheme)\n",
    "\n",
    "plt.title(\"PC Probing (Close-up) - Primary Gleason\")\n",
    "plt.legend(title='Model', labels=['BERT', 'TNLR','PubMEd BERT', 'BioBERT','Clinical BioBERT'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig('micro.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
