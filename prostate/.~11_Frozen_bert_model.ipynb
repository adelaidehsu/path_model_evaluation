{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b982597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling TuringNLRv3Tokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "import lime\n",
    "import shap\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "code_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "base_dir = \"/\".join(code_dir.split('/')[:-1])\n",
    "sys.path.append(f\"{base_dir}/code/turing/examples-raw/gluesst_finetune/\")\n",
    "sys.path.append(f\"{base_dir}/code/turing/src/\")\n",
    "\n",
    "from argparse import Namespace\n",
    "from methods.bag_of_ngrams.processing import cleanReports, cleanSplit, stripChars\n",
    "from pyfunctions.general import extractListFromDic, readJson\n",
    "from pyfunctions.pathology import extract_synoptic, fixProstateLabels\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from turing.pathology.run_classifier import MODEL_CLASSES, processors, train_path as train\n",
    "from methods.torch.processing import make_weights_for_balanced_classes\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from turing.pathology.path_utils import evaluate, extract_features, load_tnlr_base, load_tnlr_tokenizer, path_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "args = {\n",
    "    'model_type': 'tnlr', # tnlr, bert, medical_bert, pubmed_bert, biobert, clinical_biobert\n",
    "    'cache_dir': f\"{base_dir}/path_nlp_turing/output/fine_tuning/bert\", # CHANGE\n",
    "    'output_dir': f\"{base_dir}/path_nlp_turing/output/fine_tuning/bert\", # CHANGE\n",
    "    'model_name_or_path': '',\n",
    "    'task_name': 'sst-2',\n",
    "    'config_name': '',\n",
    "    'tokenizer_name': '',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'evaluate_during_training': True,\n",
    "    'max_seq_length': 512,\n",
    "    'do_lower_case': True,\n",
    "    'per_gpu_train_batch_size': 8,\n",
    "    'per_gpu_eval_batch_size': 8,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'learning_rate': 7e-6,\n",
    "    'weight_decay': 0.0,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_grad_norm': 1,\n",
    "    'num_train_epochs': 3.0,\n",
    "    'max_steps': -1,\n",
    "    'warmup_ratio': 0.2,\n",
    "    'logging_steps': 50,\n",
    "    'eval_all_checkpoints': True,\n",
    "    'no_cuda': False,\n",
    "    'overwrite_output_dir': True,\n",
    "    'seed': 42,\n",
    "    'overwrite_cache': True,\n",
    "    'metric_for_choose_best_checkpoint': None,\n",
    "    'fp16': False,\n",
    "    'fp16_opt_level': 'O1',\n",
    "    'local_rank': -1,\n",
    "    'num_train_epochs': 25, \n",
    "    'n_gpu': 1,\n",
    "    'device': 'cpu'\n",
    "}\n",
    "\n",
    "def fixLabel(data):\n",
    "    for split_ in ['train', 'val', 'test', 'dev_test']:\n",
    "        for i in range(len(data[split_])):\n",
    "            for field in ['PrimaryGleason', 'SecondaryGleason', 'TertiaryGleason']:\n",
    "                if data[split_][i]['labels'][field] == 0:\n",
    "                    data[split_][i]['labels'][field] = 'null'\n",
    "    return data\n",
    "\n",
    "# Read in data\n",
    "path = f\"{base_dir}/data/splits/prostate.json\"\n",
    "data = readJson(path)\n",
    "\n",
    "# Clean reports\n",
    "data = cleanSplit(data, stripChars)\n",
    "data['dev_test'] = cleanReports(data['dev_test'], stripChars)\n",
    "\n",
    "data = fixLabel(data)\n",
    "data = fixProstateLabels(data)\n",
    "\n",
    "# Tokenizer\n",
    "if args['model_type'] == 'bert':\n",
    "    bert_path = 'bert-base-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "elif args['model_type'] == 'medical_bert':\n",
    "    bert_path = f\"{base_dir}/models/pretrained/bert_pretrain_output_all_notes_150000/\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_path, local_files_only=True)\n",
    "elif args['model_type'] == 'medical_bert':\n",
    "    bert_path = f\"{base_dir}/models/pretrained/bert_pretrain_output_all_notes_150000/\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_path, local_files_only=True)\n",
    "elif args['model_type'] == 'pubmed_bert':\n",
    "    bert_path = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "elif args['model_type'] == 'pubmed_bert_full':\n",
    "    bert_path = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "elif args['model_type'] == 'biobert':\n",
    "    bert_path = \"dmis-lab/biobert-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "elif args['model_type'] == 'clinical_biobert':\n",
    "    bert_path = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "elif args['model_type'] == 'tnlr':\n",
    "    vocab_file = f'{code_dir}/turing/src/tnlr/tokenizer/tnlr-uncased-vocab.txt'\n",
    "    tokenizer = load_tnlr_tokenizer(vocab_file)\n",
    "    \n",
    "train_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['train']]\n",
    "val_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['val']]\n",
    "test_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['test']]\n",
    "\n",
    "kwargs = Namespace(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993fb2b",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a09c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrimaryGleason\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f157df6fb74a1fa9999ccbb689e31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.4453024453024455e-07, \"loss\": 1.8745895171165465, \"step\": 50}\n",
      "{\"learning_rate\": 4.890604890604891e-07, \"loss\": 1.8913129878044128, \"step\": 100}\n",
      "{\"learning_rate\": 7.335907335907336e-07, \"loss\": 1.9214560556411744, \"step\": 150}\n",
      "{\"learning_rate\": 9.781209781209782e-07, \"loss\": 1.8696925568580627, \"step\": 200}\n",
      "{\"learning_rate\": 1.2226512226512227e-06, \"loss\": 1.8906389379501343, \"step\": 250}\n",
      "{'acc': 0.27079303675048355}\n",
      "{\"eval_acc\": 0.27079303675048355, \"learning_rate\": 1.2666666666666667e-06, \"loss\": 0.3205408239364624, \"step\": 259}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   3%|▎         | 1/30 [03:06<1:30:11, 186.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.4671814671814672e-06, \"loss\": 1.5280623483657836, \"step\": 300}\n",
      "{\"learning_rate\": 1.7117117117117117e-06, \"loss\": 1.887594883441925, \"step\": 350}\n",
      "{\"learning_rate\": 1.9562419562419564e-06, \"loss\": 1.7930580401420593, \"step\": 400}\n",
      "{\"learning_rate\": 2.2007722007722007e-06, \"loss\": 1.8980509090423583, \"step\": 450}\n",
      "{\"learning_rate\": 2.4453024453024454e-06, \"loss\": 1.84583904504776, \"step\": 500}\n",
      "{'acc': 0.2611218568665377}\n",
      "{\"eval_acc\": 0.2611218568665377, \"learning_rate\": 2.5333333333333334e-06, \"loss\": 0.6652115249633789, \"step\": 518}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   7%|▋         | 2/30 [06:00<1:23:28, 178.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.6898326898326897e-06, \"loss\": 1.162382562160492, \"step\": 550}\n",
      "{\"learning_rate\": 2.9343629343629344e-06, \"loss\": 1.8371455550193787, \"step\": 600}\n",
      "{\"learning_rate\": 3.1788931788931787e-06, \"loss\": 1.8675964426994325, \"step\": 650}\n",
      "{\"learning_rate\": 3.4234234234234234e-06, \"loss\": 1.8342946004867553, \"step\": 700}\n",
      "{\"learning_rate\": 3.6679536679536677e-06, \"loss\": 1.792144434452057, \"step\": 750}\n",
      "{'acc': 0.2553191489361702}\n",
      "{\"eval_acc\": 0.2553191489361702, \"learning_rate\": 3.8e-06, \"loss\": 0.9695229482650757, \"step\": 777}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 3/30 [08:50<1:18:48, 175.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.912483912483913e-06, \"loss\": 0.8432932186126709, \"step\": 800}\n",
      "{\"learning_rate\": 4.157014157014157e-06, \"loss\": 1.8107656645774841, \"step\": 850}\n",
      "{\"learning_rate\": 4.401544401544401e-06, \"loss\": 1.819270691871643, \"step\": 900}\n",
      "{\"learning_rate\": 4.6460746460746465e-06, \"loss\": 1.7910908198356628, \"step\": 950}\n",
      "{\"learning_rate\": 4.890604890604891e-06, \"loss\": 1.7942587852478027, \"step\": 1000}\n",
      "{'acc': 0.26305609284332687}\n",
      "{\"eval_acc\": 0.26305609284332687, \"learning_rate\": 5.066666666666667e-06, \"loss\": 1.285618042945862, \"step\": 1036}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  13%|█▎        | 4/30 [11:41<1:15:09, 173.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.135135135135135e-06, \"loss\": 0.4945956301689148, \"step\": 1050}\n",
      "{\"learning_rate\": 5.379665379665379e-06, \"loss\": 1.7653615188598633, \"step\": 1100}\n",
      "{\"learning_rate\": 5.6241956241956245e-06, \"loss\": 1.7484693002700806, \"step\": 1150}\n",
      "{\"learning_rate\": 5.868725868725869e-06, \"loss\": 1.7377806496620178, \"step\": 1200}\n",
      "{\"learning_rate\": 6.113256113256113e-06, \"loss\": 1.760166175365448, \"step\": 1250}\n",
      "{'acc': 0.2495164410058027}\n",
      "{\"eval_acc\": 0.2495164410058027, \"learning_rate\": 6.333333333333333e-06, \"loss\": 1.5709735250473023, \"step\": 1295}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  17%|█▋        | 5/30 [14:32<1:11:52, 172.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.357786357786357e-06, \"loss\": 0.17810946941375733, \"step\": 1300}\n",
      "{\"learning_rate\": 6.6023166023166025e-06, \"loss\": 1.7378299164772033, \"step\": 1350}\n",
      "{\"learning_rate\": 6.846846846846847e-06, \"loss\": 1.728660626411438, \"step\": 1400}\n",
      "{\"learning_rate\": 7.091377091377091e-06, \"loss\": 1.7476241040229796, \"step\": 1450}\n",
      "{\"learning_rate\": 7.335907335907335e-06, \"loss\": 1.7185345911979675, \"step\": 1500}\n",
      "{\"learning_rate\": 7.5804375804375804e-06, \"loss\": 1.7113002157211303, \"step\": 1550}\n",
      "{'acc': 0.3578336557059961}\n",
      "{\"eval_acc\": 0.3578336557059961, \"learning_rate\": 7.6e-06, \"loss\": 0.1316801619529724, \"step\": 1554}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 6/30 [17:22<1:08:42, 171.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 7.543758043758044e-06, \"loss\": 1.564370334148407, \"step\": 1600}\n",
      "{\"learning_rate\": 7.482625482625483e-06, \"loss\": 1.7016924118995667, \"step\": 1650}\n",
      "{\"learning_rate\": 7.4214929214929216e-06, \"loss\": 1.7096781516075135, \"step\": 1700}\n",
      "{\"learning_rate\": 7.36036036036036e-06, \"loss\": 1.7028114008903503, \"step\": 1750}\n",
      "{\"learning_rate\": 7.2992277992277994e-06, \"loss\": 1.7040590286254882, \"step\": 1800}\n",
      "{'acc': 0.4758220502901354}\n",
      "{\"eval_acc\": 0.4758220502901354, \"learning_rate\": 7.283333333333334e-06, \"loss\": 0.44402180433273314, \"step\": 1813}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  23%|██▎       | 7/30 [20:12<1:05:34, 171.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 7.238095238095238e-06, \"loss\": 1.2503627157211303, \"step\": 1850}\n",
      "{\"learning_rate\": 7.176962676962677e-06, \"loss\": 1.6990611886978149, \"step\": 1900}\n",
      "{\"learning_rate\": 7.115830115830116e-06, \"loss\": 1.677524287700653, \"step\": 1950}\n",
      "{\"learning_rate\": 7.054697554697555e-06, \"loss\": 1.6795089054107666, \"step\": 2000}\n",
      "{\"learning_rate\": 6.993564993564994e-06, \"loss\": 1.6886595225334167, \"step\": 2050}\n",
      "{'acc': 0.3984526112185687}\n",
      "{\"eval_acc\": 0.3984526112185687, \"learning_rate\": 6.966666666666666e-06, \"loss\": 0.7395158720016479, \"step\": 2072}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  27%|██▋       | 8/30 [23:02<1:02:34, 170.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.932432432432432e-06, \"loss\": 0.9377016854286194, \"step\": 2100}\n",
      "{\"learning_rate\": 6.8712998712998715e-06, \"loss\": 1.664802803993225, \"step\": 2150}\n",
      "{\"learning_rate\": 6.81016731016731e-06, \"loss\": 1.6518063855171203, \"step\": 2200}\n",
      "{\"learning_rate\": 6.749034749034749e-06, \"loss\": 1.6768798446655273, \"step\": 2250}\n",
      "{\"learning_rate\": 6.687902187902188e-06, \"loss\": 1.646567268371582, \"step\": 2300}\n",
      "{'acc': 0.3984526112185687}\n",
      "{\"eval_acc\": 0.3984526112185687, \"learning_rate\": 6.65e-06, \"loss\": 1.0198063802719117, \"step\": 2331}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 9/30 [25:51<59:37, 170.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.626769626769626e-06, \"loss\": 0.6434251284599304, \"step\": 2350}\n",
      "{\"learning_rate\": 6.565637065637066e-06, \"loss\": 1.6723394632339477, \"step\": 2400}\n",
      "{\"learning_rate\": 6.504504504504505e-06, \"loss\": 1.6298953437805175, \"step\": 2450}\n",
      "{\"learning_rate\": 6.443371943371943e-06, \"loss\": 1.6450933742523193, \"step\": 2500}\n",
      "{\"learning_rate\": 6.382239382239382e-06, \"loss\": 1.626934416294098, \"step\": 2550}\n",
      "{'acc': 0.3617021276595745}\n",
      "{\"eval_acc\": 0.3617021276595745, \"learning_rate\": 6.333333333333333e-06, \"loss\": 1.3080182766914368, \"step\": 2590}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███▎      | 10/30 [28:41<56:41, 170.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.3211068211068214e-06, \"loss\": 0.3307780718803406, \"step\": 2600}\n",
      "{\"learning_rate\": 6.25997425997426e-06, \"loss\": 1.6259290099143981, \"step\": 2650}\n",
      "{\"learning_rate\": 6.1988416988416985e-06, \"loss\": 1.6506995558738708, \"step\": 2700}\n",
      "{\"learning_rate\": 6.137709137709138e-06, \"loss\": 1.6047440004348754, \"step\": 2750}\n",
      "{\"learning_rate\": 6.076576576576577e-06, \"loss\": 1.6049029922485352, \"step\": 2800}\n",
      "{'acc': 0.32688588007736946}\n",
      "{\"eval_acc\": 0.32688588007736946, \"learning_rate\": 6.016666666666666e-06, \"loss\": 1.5871696877479553, \"step\": 2849}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  37%|███▋      | 11/30 [31:31<53:49, 169.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.015444015444016e-06, \"loss\": 0.032578790187835695, \"step\": 2850}\n",
      "{\"learning_rate\": 5.954311454311454e-06, \"loss\": 1.6241003131866456, \"step\": 2900}\n",
      "{\"learning_rate\": 5.8931788931788935e-06, \"loss\": 1.6116048383712769, \"step\": 2950}\n",
      "{\"learning_rate\": 5.832046332046332e-06, \"loss\": 1.6156972932815552, \"step\": 3000}\n",
      "{\"learning_rate\": 5.770913770913771e-06, \"loss\": 1.6054464387893677, \"step\": 3050}\n",
      "{\"learning_rate\": 5.70978120978121e-06, \"loss\": 1.6188101720809938, \"step\": 3100}\n",
      "{'acc': 0.40618955512572535}\n",
      "{\"eval_acc\": 0.40618955512572535, \"learning_rate\": 5.7e-06, \"loss\": 0.24598644256591798, \"step\": 3108}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 12/30 [34:22<51:05, 170.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.648648648648648e-06, \"loss\": 1.349283299446106, \"step\": 3150}\n",
      "{\"learning_rate\": 5.587516087516088e-06, \"loss\": 1.5864051294326782, \"step\": 3200}\n",
      "{\"learning_rate\": 5.526383526383526e-06, \"loss\": 1.6130771493911744, \"step\": 3250}\n",
      "{\"learning_rate\": 5.465250965250965e-06, \"loss\": 1.5813260531425477, \"step\": 3300}\n",
      "{\"learning_rate\": 5.404118404118404e-06, \"loss\": 1.5528493046760559, \"step\": 3350}\n",
      "{'acc': 0.344294003868472}\n",
      "{\"eval_acc\": 0.344294003868472, \"learning_rate\": 5.383333333333334e-06, \"loss\": 0.5464781403541565, \"step\": 3367}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  43%|████▎     | 13/30 [37:12<48:16, 170.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.3429858429858435e-06, \"loss\": 1.04369975566864, \"step\": 3400}\n",
      "{\"learning_rate\": 5.281853281853282e-06, \"loss\": 1.596737802028656, \"step\": 3450}\n",
      "{\"learning_rate\": 5.2207207207207205e-06, \"loss\": 1.5981567406654358, \"step\": 3500}\n",
      "{\"learning_rate\": 5.15958815958816e-06, \"loss\": 1.5949385809898375, \"step\": 3550}\n",
      "{\"learning_rate\": 5.098455598455599e-06, \"loss\": 1.5693170428276062, \"step\": 3600}\n",
      "{'acc': 0.27079303675048355}\n",
      "{\"eval_acc\": 0.27079303675048355, \"learning_rate\": 5.066666666666667e-06, \"loss\": 0.7954877948760987, \"step\": 3626}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  47%|████▋     | 14/30 [40:03<45:27, 170.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.037323037323037e-06, \"loss\": 0.7800205826759339, \"step\": 3650}\n",
      "{\"learning_rate\": 4.976190476190476e-06, \"loss\": 1.5692455410957336, \"step\": 3700}\n",
      "{\"learning_rate\": 4.9150579150579155e-06, \"loss\": 1.5757599663734436, \"step\": 3750}\n",
      "{\"learning_rate\": 4.853925353925354e-06, \"loss\": 1.5651555109024047, \"step\": 3800}\n",
      "{\"learning_rate\": 4.7927927927927925e-06, \"loss\": 1.58541898727417, \"step\": 3850}\n",
      "{'acc': 0.28433268858800775}\n",
      "{\"eval_acc\": 0.28433268858800775, \"learning_rate\": 4.75e-06, \"loss\": 1.0931029868125917, \"step\": 3885}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 15/30 [42:53<42:37, 170.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 4.731660231660232e-06, \"loss\": 0.4616394138336182, \"step\": 3900}\n",
      "{\"learning_rate\": 4.67052767052767e-06, \"loss\": 1.5545411109924316, \"step\": 3950}\n",
      "{\"learning_rate\": 4.60939510939511e-06, \"loss\": 1.5629727888107299, \"step\": 4000}\n",
      "{\"learning_rate\": 4.548262548262548e-06, \"loss\": 1.5374227285385131, \"step\": 4050}\n",
      "{\"learning_rate\": 4.487129987129987e-06, \"loss\": 1.5487101197242736, \"step\": 4100}\n",
      "{'acc': 0.3365570599613153}\n",
      "{\"eval_acc\": 0.3365570599613153, \"learning_rate\": 4.433333333333334e-06, \"loss\": 1.3671917557716369, \"step\": 4144}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  53%|█████▎    | 16/30 [45:44<39:48, 170.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 4.425997425997426e-06, \"loss\": 0.18533512830734253, \"step\": 4150}\n",
      "{\"learning_rate\": 4.364864864864865e-06, \"loss\": 1.561918249130249, \"step\": 4200}\n",
      "{\"learning_rate\": 4.303732303732304e-06, \"loss\": 1.5756062483787536, \"step\": 4250}\n",
      "{\"learning_rate\": 4.2425997425997425e-06, \"loss\": 1.5468743062019348, \"step\": 4300}\n",
      "{\"learning_rate\": 4.181467181467182e-06, \"loss\": 1.567917718887329, \"step\": 4350}\n",
      "{\"learning_rate\": 4.12033462033462e-06, \"loss\": 1.4943590569496155, \"step\": 4400}\n",
      "{'acc': 0.40425531914893614}\n",
      "{\"eval_acc\": 0.40425531914893614, \"learning_rate\": 4.1166666666666665e-06, \"loss\": 0.09324126958847045, \"step\": 4403}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  57%|█████▋    | 17/30 [48:35<36:57, 170.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 4.059202059202059e-06, \"loss\": 1.453589460849762, \"step\": 4450}\n",
      "{\"learning_rate\": 3.998069498069498e-06, \"loss\": 1.5331479167938233, \"step\": 4500}\n",
      "{\"learning_rate\": 3.9369369369369376e-06, \"loss\": 1.5439159488677978, \"step\": 4550}\n",
      "{\"learning_rate\": 3.875804375804375e-06, \"loss\": 1.544573793411255, \"step\": 4600}\n",
      "{\"learning_rate\": 3.8146718146718146e-06, \"loss\": 1.5562872362136841, \"step\": 4650}\n",
      "{'acc': 0.41779497098646035}\n",
      "{\"eval_acc\": 0.41779497098646035, \"learning_rate\": 3.8e-06, \"loss\": 0.37356488704681395, \"step\": 4662}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 18/30 [51:26<34:07, 170.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.7535392535392535e-06, \"loss\": 1.1492267203330995, \"step\": 4700}\n",
      "{\"learning_rate\": 3.6924066924066924e-06, \"loss\": 1.5210960340499877, \"step\": 4750}\n",
      "{\"learning_rate\": 3.6312741312741313e-06, \"loss\": 1.5083176279067994, \"step\": 4800}\n",
      "{\"learning_rate\": 3.5701415701415703e-06, \"loss\": 1.5134837293624879, \"step\": 4850}\n",
      "{\"learning_rate\": 3.509009009009009e-06, \"loss\": 1.517561309337616, \"step\": 4900}\n",
      "{'acc': 0.3984526112185687}\n",
      "{\"eval_acc\": 0.3984526112185687, \"learning_rate\": 3.483333333333333e-06, \"loss\": 0.6461878490447998, \"step\": 4921}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  63%|██████▎   | 19/30 [54:16<31:17, 170.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.4478764478764477e-06, \"loss\": 0.8872838544845582, \"step\": 4950}\n",
      "{\"learning_rate\": 3.386743886743887e-06, \"loss\": 1.5103185439109803, \"step\": 5000}\n",
      "{\"learning_rate\": 3.3256113256113256e-06, \"loss\": 1.5256716322898864, \"step\": 5050}\n",
      "{\"learning_rate\": 3.2644787644787645e-06, \"loss\": 1.5296693634986878, \"step\": 5100}\n",
      "{\"learning_rate\": 3.2033462033462034e-06, \"loss\": 1.5404730439186096, \"step\": 5150}\n",
      "{'acc': 0.3829787234042553}\n",
      "{\"eval_acc\": 0.3829787234042553, \"learning_rate\": 3.1666666666666667e-06, \"loss\": 0.9066585850715637, \"step\": 5180}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████▋   | 20/30 [57:07<28:27, 170.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.1422136422136424e-06, \"loss\": 0.5955559611320496, \"step\": 5200}\n",
      "{\"learning_rate\": 3.0810810810810813e-06, \"loss\": 1.5180141925811768, \"step\": 5250}\n",
      "{\"learning_rate\": 3.01994851994852e-06, \"loss\": 1.507013466358185, \"step\": 5300}\n",
      "{\"learning_rate\": 2.9588159588159587e-06, \"loss\": 1.5040219497680665, \"step\": 5350}\n",
      "{\"learning_rate\": 2.8976833976833976e-06, \"loss\": 1.483647437095642, \"step\": 5400}\n",
      "{'acc': 0.3733075435203095}\n",
      "{\"eval_acc\": 0.3733075435203095, \"learning_rate\": 2.85e-06, \"loss\": 1.1863038444519043, \"step\": 5439}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|███████   | 21/30 [59:58<25:36, 170.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.8365508365508366e-06, \"loss\": 0.32623915433883666, \"step\": 5450}\n",
      "{\"learning_rate\": 2.775418275418275e-06, \"loss\": 1.5286940336227417, \"step\": 5500}\n",
      "{\"learning_rate\": 2.7142857142857144e-06, \"loss\": 1.5069025373458862, \"step\": 5550}\n",
      "{\"learning_rate\": 2.653153153153153e-06, \"loss\": 1.5054471373558045, \"step\": 5600}\n",
      "{\"learning_rate\": 2.5920205920205923e-06, \"loss\": 1.5175598740577698, \"step\": 5650}\n",
      "{'acc': 0.3617021276595745}\n",
      "{\"eval_acc\": 0.3617021276595745, \"learning_rate\": 2.5333333333333334e-06, \"loss\": 1.4548501205444335, \"step\": 5698}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  73%|███████▎  | 22/30 [1:02:49<22:45, 170.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.530888030888031e-06, \"loss\": 0.06220101594924927, \"step\": 5700}\n",
      "{\"learning_rate\": 2.4697554697554697e-06, \"loss\": 1.4993782591819764, \"step\": 5750}\n",
      "{\"learning_rate\": 2.4086229086229087e-06, \"loss\": 1.4982675433158874, \"step\": 5800}\n",
      "{\"learning_rate\": 2.3474903474903476e-06, \"loss\": 1.4963788294792175, \"step\": 5850}\n",
      "{\"learning_rate\": 2.286357786357786e-06, \"loss\": 1.520427062511444, \"step\": 5900}\n",
      "{\"learning_rate\": 2.2252252252252254e-06, \"loss\": 1.4775885152816772, \"step\": 5950}\n",
      "{'acc': 0.37524177949709864}\n",
      "{\"eval_acc\": 0.37524177949709864, \"learning_rate\": 2.216666666666667e-06, \"loss\": 0.21782848596572876, \"step\": 5957}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  77%|███████▋  | 23/30 [1:05:39<19:54, 170.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.164092664092664e-06, \"loss\": 1.2887107586860658, \"step\": 6000}\n",
      "{\"learning_rate\": 2.1029601029601033e-06, \"loss\": 1.5107821750640869, \"step\": 6050}\n",
      "{\"learning_rate\": 2.041827541827542e-06, \"loss\": 1.5290317010879517, \"step\": 6100}\n",
      "{\"learning_rate\": 1.9806949806949807e-06, \"loss\": 1.5039440655708314, \"step\": 6150}\n",
      "{\"learning_rate\": 1.9195624195624197e-06, \"loss\": 1.503435299396515, \"step\": 6200}\n",
      "{'acc': 0.3771760154738878}\n",
      "{\"eval_acc\": 0.3771760154738878, \"learning_rate\": 1.9e-06, \"loss\": 0.48959864139556886, \"step\": 6216}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 24/30 [1:08:30<17:04, 170.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.8584298584298584e-06, \"loss\": 1.0306268644332885, \"step\": 6250}\n",
      "{\"learning_rate\": 1.7972972972972973e-06, \"loss\": 1.4881881022453307, \"step\": 6300}\n",
      "{\"learning_rate\": 1.7361647361647362e-06, \"loss\": 1.5082632303237915, \"step\": 6350}\n",
      "{\"learning_rate\": 1.6750321750321752e-06, \"loss\": 1.499279305934906, \"step\": 6400}\n",
      "{\"learning_rate\": 1.6138996138996139e-06, \"loss\": 1.5112642598152162, \"step\": 6450}\n",
      "{'acc': 0.3617021276595745}\n",
      "{\"eval_acc\": 0.3617021276595745, \"learning_rate\": 1.5833333333333333e-06, \"loss\": 0.7411232161521911, \"step\": 6475}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  83%|████████▎ | 25/30 [1:11:21<14:13, 170.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.5527670527670528e-06, \"loss\": 0.7574433016777039, \"step\": 6500}\n",
      "{\"learning_rate\": 1.4916344916344917e-06, \"loss\": 1.4996034574508668, \"step\": 6550}\n",
      "{\"learning_rate\": 1.4305019305019307e-06, \"loss\": 1.480364775657654, \"step\": 6600}\n",
      "{\"learning_rate\": 1.3693693693693694e-06, \"loss\": 1.5084811186790466, \"step\": 6650}\n",
      "{\"learning_rate\": 1.3082368082368083e-06, \"loss\": 1.4905131721496583, \"step\": 6700}\n",
      "{'acc': 0.3771760154738878}\n",
      "{\"eval_acc\": 0.3771760154738878, \"learning_rate\": 1.2666666666666667e-06, \"loss\": 1.0254354453086854, \"step\": 6734}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  87%|████████▋ | 26/30 [1:14:11<11:22, 170.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.2471042471042472e-06, \"loss\": 0.4905950427055359, \"step\": 6750}\n",
      "{\"learning_rate\": 1.1859716859716862e-06, \"loss\": 1.4992468166351318, \"step\": 6800}\n",
      "{\"learning_rate\": 1.1248391248391247e-06, \"loss\": 1.4933645915985108, \"step\": 6850}\n",
      "{\"learning_rate\": 1.0637065637065636e-06, \"loss\": 1.4861773443222046, \"step\": 6900}\n",
      "{\"learning_rate\": 1.0025740025740025e-06, \"loss\": 1.466964728832245, \"step\": 6950}\n",
      "{'acc': 0.3694390715667311}\n",
      "{\"eval_acc\": 0.3694390715667311, \"learning_rate\": 9.5e-07, \"loss\": 1.2986743283271789, \"step\": 6993}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|█████████ | 27/30 [1:17:02<08:32, 170.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 9.414414414414415e-07, \"loss\": 0.20756663799285888, \"step\": 7000}\n",
      "{\"learning_rate\": 8.803088803088804e-07, \"loss\": 1.5036402750015259, \"step\": 7050}\n",
      "{\"learning_rate\": 8.191763191763191e-07, \"loss\": 1.4881804394721985, \"step\": 7100}\n",
      "{\"learning_rate\": 7.58043758043758e-07, \"loss\": 1.504110472202301, \"step\": 7150}\n",
      "{\"learning_rate\": 6.969111969111969e-07, \"loss\": 1.4681035494804382, \"step\": 7200}\n",
      "{\"learning_rate\": 6.357786357786358e-07, \"loss\": 1.4798837327957153, \"step\": 7250}\n",
      "{'acc': 0.3771760154738878}\n",
      "{\"eval_acc\": 0.3771760154738878, \"learning_rate\": 6.333333333333333e-07, \"loss\": 0.06299951314926147, \"step\": 7252}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  93%|█████████▎| 28/30 [1:19:53<05:41, 170.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.746460746460746e-07, \"loss\": 1.4309077477455139, \"step\": 7300}\n",
      "{\"learning_rate\": 5.135135135135135e-07, \"loss\": 1.489315264225006, \"step\": 7350}\n",
      "{\"learning_rate\": 4.5238095238095237e-07, \"loss\": 1.467853262424469, \"step\": 7400}\n",
      "{\"learning_rate\": 3.9124839124839125e-07, \"loss\": 1.4628421115875243, \"step\": 7450}\n",
      "{\"learning_rate\": 3.301158301158301e-07, \"loss\": 1.4879957914352417, \"step\": 7500}\n",
      "{'acc': 0.3771760154738878}\n",
      "{\"eval_acc\": 0.3771760154738878, \"learning_rate\": 3.1666666666666667e-07, \"loss\": 0.34146456241607664, \"step\": 7511}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  97%|█████████▋| 29/30 [1:22:43<02:50, 170.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.68983268983269e-07, \"loss\": 1.173017876148224, \"step\": 7550}\n",
      "{\"learning_rate\": 2.0785070785070785e-07, \"loss\": 1.491936764717102, \"step\": 7600}\n",
      "{\"learning_rate\": 1.4671814671814672e-07, \"loss\": 1.4805126214027404, \"step\": 7650}\n",
      "{\"learning_rate\": 8.558558558558559e-08, \"loss\": 1.4609050822257996, \"step\": 7700}\n",
      "{\"learning_rate\": 2.4453024453024453e-08, \"loss\": 1.468930833339691, \"step\": 7750}\n",
      "{'acc': 0.379110251450677}\n",
      "{\"eval_acc\": 0.379110251450677, \"learning_rate\": 0.0, \"loss\": 0.5894289660453796, \"step\": 7770}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/PrimaryGleason/epoch-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [1:25:34<00:00, 171.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performance {'acc': 0.4758220502901354, 'checkpoint': 'epoch-7'}\n",
      "SecondaryGleason\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.4453024453024455e-07, \"loss\": 1.8463748240470885, \"step\": 50}\n",
      "{\"learning_rate\": 4.890604890604891e-07, \"loss\": 1.9090657830238342, \"step\": 100}\n",
      "{\"learning_rate\": 7.335907335907336e-07, \"loss\": 1.9113142132759093, \"step\": 150}\n",
      "{\"learning_rate\": 9.781209781209782e-07, \"loss\": 1.8963112711906434, \"step\": 200}\n",
      "{\"learning_rate\": 1.2226512226512227e-06, \"loss\": 1.9243201088905335, \"step\": 250}\n",
      "{'acc': 0.0}\n",
      "{\"eval_acc\": 0.0, \"learning_rate\": 1.2666666666666667e-06, \"loss\": 0.3230728268623352, \"step\": 259}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   3%|▎         | 1/30 [02:50<1:22:15, 170.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.4671814671814672e-06, \"loss\": 1.5450809764862061, \"step\": 300}\n",
      "{\"learning_rate\": 1.7117117117117117e-06, \"loss\": 1.9130544424057008, \"step\": 350}\n",
      "{\"learning_rate\": 1.9562419562419564e-06, \"loss\": 1.9019347834587097, \"step\": 400}\n",
      "{\"learning_rate\": 2.2007722007722007e-06, \"loss\": 1.8850143623352051, \"step\": 450}\n",
      "{\"learning_rate\": 2.4453024453024454e-06, \"loss\": 1.9000126814842224, \"step\": 500}\n",
      "{'acc': 0.0}\n",
      "{\"eval_acc\": 0.0, \"learning_rate\": 2.5333333333333334e-06, \"loss\": 0.6894643092155457, \"step\": 518}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   7%|▋         | 2/30 [05:40<1:19:32, 170.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.6898326898326897e-06, \"loss\": 1.2024662518501281, \"step\": 550}\n",
      "{\"learning_rate\": 2.9343629343629344e-06, \"loss\": 1.8918159008026123, \"step\": 600}\n",
      "{\"learning_rate\": 3.1788931788931787e-06, \"loss\": 1.8948252940177917, \"step\": 650}\n",
      "{\"learning_rate\": 3.4234234234234234e-06, \"loss\": 1.8752863788604737, \"step\": 700}\n",
      "{\"learning_rate\": 3.6679536679536677e-06, \"loss\": 1.8255097365379334, \"step\": 750}\n",
      "{'acc': 0.0}\n",
      "{\"eval_acc\": 0.0, \"learning_rate\": 3.8e-06, \"loss\": 0.9889277696609498, \"step\": 777}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 3/30 [08:31<1:16:43, 170.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.912483912483913e-06, \"loss\": 0.850654308795929, \"step\": 800}\n",
      "{\"learning_rate\": 4.157014157014157e-06, \"loss\": 1.836374385356903, \"step\": 850}\n",
      "{\"learning_rate\": 4.401544401544401e-06, \"loss\": 1.8379748463630676, \"step\": 900}\n",
      "{\"learning_rate\": 4.6460746460746465e-06, \"loss\": 1.8198600006103516, \"step\": 950}\n",
      "{\"learning_rate\": 4.890604890604891e-06, \"loss\": 1.8289884209632874, \"step\": 1000}\n",
      "{'acc': 0.015473887814313346}\n",
      "{\"eval_acc\": 0.015473887814313346, \"learning_rate\": 5.066666666666667e-06, \"loss\": 1.2979151940345763, \"step\": 1036}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  13%|█▎        | 4/30 [11:21<1:13:46, 170.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.135135135135135e-06, \"loss\": 0.50588871717453, \"step\": 1050}\n",
      "{\"learning_rate\": 5.379665379665379e-06, \"loss\": 1.8064404129981995, \"step\": 1100}\n",
      "{\"learning_rate\": 5.6241956241956245e-06, \"loss\": 1.799101758003235, \"step\": 1150}\n",
      "{\"learning_rate\": 5.868725868725869e-06, \"loss\": 1.7820468950271606, \"step\": 1200}\n",
      "{\"learning_rate\": 6.113256113256113e-06, \"loss\": 1.783625874519348, \"step\": 1250}\n",
      "{'acc': 0.059961315280464215}\n",
      "{\"eval_acc\": 0.059961315280464215, \"learning_rate\": 6.333333333333333e-06, \"loss\": 1.5934216451644898, \"step\": 1295}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  17%|█▋        | 5/30 [14:10<1:10:48, 169.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.357786357786357e-06, \"loss\": 0.18272708654403685, \"step\": 1300}\n",
      "{\"learning_rate\": 6.6023166023166025e-06, \"loss\": 1.7625658369064332, \"step\": 1350}\n",
      "{\"learning_rate\": 6.846846846846847e-06, \"loss\": 1.7581905579566957, \"step\": 1400}\n",
      "{\"learning_rate\": 7.091377091377091e-06, \"loss\": 1.7578238129615784, \"step\": 1450}\n",
      "{\"learning_rate\": 7.335907335907335e-06, \"loss\": 1.7491144132614136, \"step\": 1500}\n",
      "{\"learning_rate\": 7.5804375804375804e-06, \"loss\": 1.7513586711883544, \"step\": 1550}\n",
      "{'acc': 0.15087040618955513}\n",
      "{\"eval_acc\": 0.15087040618955513, \"learning_rate\": 7.6e-06, \"loss\": 0.14136572122573854, \"step\": 1554}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 6/30 [17:00<1:07:56, 169.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 7.543758043758044e-06, \"loss\": 1.6094491910934448, \"step\": 1600}\n",
      "{\"learning_rate\": 7.482625482625483e-06, \"loss\": 1.7307346057891846, \"step\": 1650}\n",
      "{\"learning_rate\": 7.4214929214929216e-06, \"loss\": 1.7357102870941161, \"step\": 1700}\n",
      "{\"learning_rate\": 7.36036036036036e-06, \"loss\": 1.737641041278839, \"step\": 1750}\n",
      "{\"learning_rate\": 7.2992277992277994e-06, \"loss\": 1.7206733965873717, \"step\": 1800}\n",
      "{'acc': 0.14119922630560927}\n",
      "{\"eval_acc\": 0.14119922630560927, \"learning_rate\": 7.283333333333334e-06, \"loss\": 0.4575686478614807, \"step\": 1813}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  23%|██▎       | 7/30 [19:50<1:05:07, 169.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 7.238095238095238e-06, \"loss\": 1.2732971692085266, \"step\": 1850}\n",
      "{\"learning_rate\": 7.176962676962677e-06, \"loss\": 1.714287474155426, \"step\": 1900}\n",
      "{\"learning_rate\": 7.115830115830116e-06, \"loss\": 1.689680244922638, \"step\": 1950}\n",
      "{\"learning_rate\": 7.054697554697555e-06, \"loss\": 1.7077969336509704, \"step\": 2000}\n",
      "{\"learning_rate\": 6.993564993564994e-06, \"loss\": 1.6998164916038514, \"step\": 2050}\n",
      "{'acc': 0.1760154738878143}\n",
      "{\"eval_acc\": 0.1760154738878143, \"learning_rate\": 6.966666666666666e-06, \"loss\": 0.7458102083206177, \"step\": 2072}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  27%|██▋       | 8/30 [22:40<1:02:22, 170.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.932432432432432e-06, \"loss\": 0.9485103631019592, \"step\": 2100}\n",
      "{\"learning_rate\": 6.8712998712998715e-06, \"loss\": 1.7019516587257386, \"step\": 2150}\n",
      "{\"learning_rate\": 6.81016731016731e-06, \"loss\": 1.6802839016914368, \"step\": 2200}\n",
      "{\"learning_rate\": 6.749034749034749e-06, \"loss\": 1.6840798139572144, \"step\": 2250}\n",
      "{\"learning_rate\": 6.687902187902188e-06, \"loss\": 1.6777931904792787, \"step\": 2300}\n",
      "{'acc': 0.23404255319148937}\n",
      "{\"eval_acc\": 0.23404255319148937, \"learning_rate\": 6.65e-06, \"loss\": 1.0428412413597108, \"step\": 2331}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 9/30 [25:30<59:28, 169.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.626769626769626e-06, \"loss\": 0.6440564489364624, \"step\": 2350}\n",
      "{\"learning_rate\": 6.565637065637066e-06, \"loss\": 1.6867199921607972, \"step\": 2400}\n",
      "{\"learning_rate\": 6.504504504504505e-06, \"loss\": 1.6721364068984985, \"step\": 2450}\n",
      "{\"learning_rate\": 6.443371943371943e-06, \"loss\": 1.673760747909546, \"step\": 2500}\n",
      "{\"learning_rate\": 6.382239382239382e-06, \"loss\": 1.6602849197387695, \"step\": 2550}\n",
      "{'acc': 0.2495164410058027}\n",
      "{\"eval_acc\": 0.2495164410058027, \"learning_rate\": 6.333333333333333e-06, \"loss\": 1.318174602985382, \"step\": 2590}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███▎      | 10/30 [28:20<56:40, 170.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.3211068211068214e-06, \"loss\": 0.3391366147994995, \"step\": 2600}\n",
      "{\"learning_rate\": 6.25997425997426e-06, \"loss\": 1.646883716583252, \"step\": 2650}\n",
      "{\"learning_rate\": 6.1988416988416985e-06, \"loss\": 1.6614654684066772, \"step\": 2700}\n",
      "{\"learning_rate\": 6.137709137709138e-06, \"loss\": 1.6457014751434327, \"step\": 2750}\n",
      "{\"learning_rate\": 6.076576576576577e-06, \"loss\": 1.6426190280914306, \"step\": 2800}\n",
      "{'acc': 0.22243713733075435}\n",
      "{\"eval_acc\": 0.22243713733075435, \"learning_rate\": 6.016666666666666e-06, \"loss\": 1.5977146482467652, \"step\": 2849}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  37%|███▋      | 11/30 [31:11<53:53, 170.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.015444015444016e-06, \"loss\": 0.030278422832489015, \"step\": 2850}\n",
      "{\"learning_rate\": 5.954311454311454e-06, \"loss\": 1.6429268980026246, \"step\": 2900}\n",
      "{\"learning_rate\": 5.8931788931788935e-06, \"loss\": 1.640279929637909, \"step\": 2950}\n",
      "{\"learning_rate\": 5.832046332046332e-06, \"loss\": 1.637984917163849, \"step\": 3000}\n",
      "{\"learning_rate\": 5.770913770913771e-06, \"loss\": 1.6179189229011535, \"step\": 3050}\n",
      "{\"learning_rate\": 5.70978120978121e-06, \"loss\": 1.642211287021637, \"step\": 3100}\n",
      "{'acc': 0.15473887814313347}\n",
      "{\"eval_acc\": 0.15473887814313347, \"learning_rate\": 5.7e-06, \"loss\": 0.2588012719154358, \"step\": 3108}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 12/30 [34:01<51:01, 170.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.648648648648648e-06, \"loss\": 1.375842251777649, \"step\": 3150}\n",
      "{\"learning_rate\": 5.587516087516088e-06, \"loss\": 1.6174088191986085, \"step\": 3200}\n",
      "{\"learning_rate\": 5.526383526383526e-06, \"loss\": 1.6268030834197997, \"step\": 3250}\n",
      "{\"learning_rate\": 5.465250965250965e-06, \"loss\": 1.6093907976150512, \"step\": 3300}\n",
      "{\"learning_rate\": 5.404118404118404e-06, \"loss\": 1.604977135658264, \"step\": 3350}\n",
      "{'acc': 0.16247582205029013}\n",
      "{\"eval_acc\": 0.16247582205029013, \"learning_rate\": 5.383333333333334e-06, \"loss\": 0.5672052240371704, \"step\": 3367}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  43%|████▎     | 13/30 [36:50<48:08, 169.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.3429858429858435e-06, \"loss\": 1.0738053965568541, \"step\": 3400}\n",
      "{\"learning_rate\": 5.281853281853282e-06, \"loss\": 1.6131325006484984, \"step\": 3450}\n",
      "{\"learning_rate\": 5.2207207207207205e-06, \"loss\": 1.6013534593582153, \"step\": 3500}\n",
      "{\"learning_rate\": 5.15958815958816e-06, \"loss\": 1.6121040081977844, \"step\": 3550}\n",
      "{\"learning_rate\": 5.098455598455599e-06, \"loss\": 1.5828857159614562, \"step\": 3600}\n",
      "{'acc': 0.21470019342359767}\n",
      "{\"eval_acc\": 0.21470019342359767, \"learning_rate\": 5.066666666666667e-06, \"loss\": 0.8139203953742981, \"step\": 3626}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  47%|████▋     | 14/30 [39:39<45:15, 169.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.037323037323037e-06, \"loss\": 0.7767272186279297, \"step\": 3650}\n",
      "{\"learning_rate\": 4.976190476190476e-06, \"loss\": 1.5883803701400756, \"step\": 3700}\n",
      "{\"learning_rate\": 4.9150579150579155e-06, \"loss\": 1.6048348569869995, \"step\": 3750}\n",
      "{\"learning_rate\": 4.853925353925354e-06, \"loss\": 1.6011941242218017, \"step\": 3800}\n",
      "{\"learning_rate\": 4.7927927927927925e-06, \"loss\": 1.600697522163391, \"step\": 3850}\n",
      "{'acc': 0.18375241779497098}\n",
      "{\"eval_acc\": 0.18375241779497098, \"learning_rate\": 4.75e-06, \"loss\": 1.0956856727600097, \"step\": 3885}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 15/30 [42:29<42:24, 169.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 4.731660231660232e-06, \"loss\": 0.46277326345443726, \"step\": 3900}\n",
      "{\"learning_rate\": 4.67052767052767e-06, \"loss\": 1.5598179054260255, \"step\": 3950}\n",
      "{\"learning_rate\": 4.60939510939511e-06, \"loss\": 1.5767599773406982, \"step\": 4000}\n",
      "{\"learning_rate\": 4.548262548262548e-06, \"loss\": 1.5746303248405455, \"step\": 4050}\n",
      "{\"learning_rate\": 4.487129987129987e-06, \"loss\": 1.572772421836853, \"step\": 4100}\n",
      "{'acc': 0.18375241779497098}\n",
      "{\"eval_acc\": 0.18375241779497098, \"learning_rate\": 4.433333333333334e-06, \"loss\": 1.3882594871520997, \"step\": 4144}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  53%|█████▎    | 16/30 [45:18<39:33, 169.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 4.425997425997426e-06, \"loss\": 0.18941853761672975, \"step\": 4150}\n",
      "{\"learning_rate\": 4.364864864864865e-06, \"loss\": 1.5897228813171387, \"step\": 4200}\n",
      "{\"learning_rate\": 4.303732303732304e-06, \"loss\": 1.570316047668457, \"step\": 4250}\n",
      "{\"learning_rate\": 4.2425997425997425e-06, \"loss\": 1.5705894613265992, \"step\": 4300}\n",
      "{\"learning_rate\": 4.181467181467182e-06, \"loss\": 1.571701476573944, \"step\": 4350}\n",
      "{\"learning_rate\": 4.12033462033462e-06, \"loss\": 1.5318743205070495, \"step\": 4400}\n",
      "{'acc': 0.14313346228239845}\n",
      "{\"eval_acc\": 0.14313346228239845, \"learning_rate\": 4.1166666666666665e-06, \"loss\": 0.09852461099624633, \"step\": 4403}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  57%|█████▋    | 17/30 [48:07<36:42, 169.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 4.059202059202059e-06, \"loss\": 1.487004244327545, \"step\": 4450}\n",
      "{\"learning_rate\": 3.998069498069498e-06, \"loss\": 1.5658991146087646, \"step\": 4500}\n",
      "{\"learning_rate\": 3.9369369369369376e-06, \"loss\": 1.552085382938385, \"step\": 4550}\n",
      "{\"learning_rate\": 3.875804375804375e-06, \"loss\": 1.5911951780319213, \"step\": 4600}\n",
      "{\"learning_rate\": 3.8146718146718146e-06, \"loss\": 1.5405976486206054, \"step\": 4650}\n",
      "{'acc': 0.16634429400386846}\n",
      "{\"eval_acc\": 0.16634429400386846, \"learning_rate\": 3.8e-06, \"loss\": 0.3785107755661011, \"step\": 4662}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 18/30 [50:57<33:52, 169.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.7535392535392535e-06, \"loss\": 1.1832833456993104, \"step\": 4700}\n",
      "{\"learning_rate\": 3.6924066924066924e-06, \"loss\": 1.5556727170944213, \"step\": 4750}\n",
      "{\"learning_rate\": 3.6312741312741313e-06, \"loss\": 1.540971655845642, \"step\": 4800}\n",
      "{\"learning_rate\": 3.5701415701415703e-06, \"loss\": 1.5343829345703126, \"step\": 4850}\n",
      "{\"learning_rate\": 3.509009009009009e-06, \"loss\": 1.5438216710090638, \"step\": 4900}\n",
      "{'acc': 0.1895551257253385}\n",
      "{\"eval_acc\": 0.1895551257253385, \"learning_rate\": 3.483333333333333e-06, \"loss\": 0.6599486374855041, \"step\": 4921}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  63%|██████▎   | 19/30 [53:46<31:02, 169.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.4478764478764477e-06, \"loss\": 0.8988898468017578, \"step\": 4950}\n",
      "{\"learning_rate\": 3.386743886743887e-06, \"loss\": 1.545458767414093, \"step\": 5000}\n",
      "{\"learning_rate\": 3.3256113256113256e-06, \"loss\": 1.5598657393455506, \"step\": 5050}\n",
      "{\"learning_rate\": 3.2644787644787645e-06, \"loss\": 1.5617958831787109, \"step\": 5100}\n",
      "{\"learning_rate\": 3.2033462033462034e-06, \"loss\": 1.5664316177368165, \"step\": 5150}\n",
      "{'acc': 0.18568665377176016}\n",
      "{\"eval_acc\": 0.18568665377176016, \"learning_rate\": 3.1666666666666667e-06, \"loss\": 0.9119297981262207, \"step\": 5180}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████▋   | 20/30 [56:35<28:13, 169.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.1422136422136424e-06, \"loss\": 0.5974963569641113, \"step\": 5200}\n",
      "{\"learning_rate\": 3.0810810810810813e-06, \"loss\": 1.5516533398628234, \"step\": 5250}\n",
      "{\"learning_rate\": 3.01994851994852e-06, \"loss\": 1.5276923632621766, \"step\": 5300}\n",
      "{\"learning_rate\": 2.9588159588159587e-06, \"loss\": 1.5585739827156067, \"step\": 5350}\n",
      "{\"learning_rate\": 2.8976833976833976e-06, \"loss\": 1.5246922540664674, \"step\": 5400}\n",
      "{'acc': 0.172147001934236}\n",
      "{\"eval_acc\": 0.172147001934236, \"learning_rate\": 2.85e-06, \"loss\": 1.21737122297287, \"step\": 5439}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|███████   | 21/30 [59:24<25:23, 169.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.8365508365508366e-06, \"loss\": 0.3324369692802429, \"step\": 5450}\n",
      "{\"learning_rate\": 2.775418275418275e-06, \"loss\": 1.5495255517959594, \"step\": 5500}\n",
      "{\"learning_rate\": 2.7142857142857144e-06, \"loss\": 1.5350789189338685, \"step\": 5550}\n",
      "{\"learning_rate\": 2.653153153153153e-06, \"loss\": 1.5193823337554933, \"step\": 5600}\n",
      "{\"learning_rate\": 2.5920205920205923e-06, \"loss\": 1.5455112767219543, \"step\": 5650}\n",
      "{'acc': 0.16247582205029013}\n",
      "{\"eval_acc\": 0.16247582205029013, \"learning_rate\": 2.5333333333333334e-06, \"loss\": 1.4848990559577941, \"step\": 5698}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  73%|███████▎  | 22/30 [1:02:14<22:34, 169.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.530888030888031e-06, \"loss\": 0.059307849407196044, \"step\": 5700}\n",
      "{\"learning_rate\": 2.4697554697554697e-06, \"loss\": 1.5505581927299499, \"step\": 5750}\n",
      "{\"learning_rate\": 2.4086229086229087e-06, \"loss\": 1.5232365369796752, \"step\": 5800}\n",
      "{\"learning_rate\": 2.3474903474903476e-06, \"loss\": 1.5096850204467773, \"step\": 5850}\n",
      "{\"learning_rate\": 2.286357786357786e-06, \"loss\": 1.5367565727233887, \"step\": 5900}\n",
      "{\"learning_rate\": 2.2252252252252254e-06, \"loss\": 1.5116461491584778, \"step\": 5950}\n",
      "{'acc': 0.14893617021276595}\n",
      "{\"eval_acc\": 0.14893617021276595, \"learning_rate\": 2.216666666666667e-06, \"loss\": 0.2133375859260559, \"step\": 5957}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  77%|███████▋  | 23/30 [1:05:04<19:48, 169.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.164092664092664e-06, \"loss\": 1.3165128755569457, \"step\": 6000}\n",
      "{\"learning_rate\": 2.1029601029601033e-06, \"loss\": 1.530268235206604, \"step\": 6050}\n",
      "{\"learning_rate\": 2.041827541827542e-06, \"loss\": 1.5367282104492188, \"step\": 6100}\n",
      "{\"learning_rate\": 1.9806949806949807e-06, \"loss\": 1.5495429039001465, \"step\": 6150}\n",
      "{\"learning_rate\": 1.9195624195624197e-06, \"loss\": 1.5244662380218506, \"step\": 6200}\n",
      "{'acc': 0.1450676982591876}\n",
      "{\"eval_acc\": 0.1450676982591876, \"learning_rate\": 1.9e-06, \"loss\": 0.4894232678413391, \"step\": 6216}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 24/30 [1:07:57<17:03, 170.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.8584298584298584e-06, \"loss\": 1.0244734477996826, \"step\": 6250}\n",
      "{\"learning_rate\": 1.7972972972972973e-06, \"loss\": 1.5075274515151977, \"step\": 6300}\n",
      "{\"learning_rate\": 1.7361647361647362e-06, \"loss\": 1.5250641131401061, \"step\": 6350}\n",
      "{\"learning_rate\": 1.6750321750321752e-06, \"loss\": 1.5201699447631836, \"step\": 6400}\n",
      "{\"learning_rate\": 1.6138996138996139e-06, \"loss\": 1.516431953907013, \"step\": 6450}\n",
      "{'acc': 0.13346228239845262}\n",
      "{\"eval_acc\": 0.13346228239845262, \"learning_rate\": 1.5833333333333333e-06, \"loss\": 0.7625369882583618, \"step\": 6475}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  83%|████████▎ | 25/30 [1:10:48<14:14, 170.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.5527670527670528e-06, \"loss\": 0.7708959221839905, \"step\": 6500}\n",
      "{\"learning_rate\": 1.4916344916344917e-06, \"loss\": 1.5167370557785034, \"step\": 6550}\n",
      "{\"learning_rate\": 1.4305019305019307e-06, \"loss\": 1.4930480551719665, \"step\": 6600}\n",
      "{\"learning_rate\": 1.3693693693693694e-06, \"loss\": 1.5149549269676208, \"step\": 6650}\n",
      "{\"learning_rate\": 1.3082368082368083e-06, \"loss\": 1.4911660242080689, \"step\": 6700}\n",
      "{'acc': 0.12572533849129594}\n",
      "{\"eval_acc\": 0.12572533849129594, \"learning_rate\": 1.2666666666666667e-06, \"loss\": 1.0474271202087402, \"step\": 6734}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  87%|████████▋ | 26/30 [1:13:39<11:23, 170.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.2471042471042472e-06, \"loss\": 0.4961884617805481, \"step\": 6750}\n",
      "{\"learning_rate\": 1.1859716859716862e-06, \"loss\": 1.5066755843162536, \"step\": 6800}\n",
      "{\"learning_rate\": 1.1248391248391247e-06, \"loss\": 1.520592532157898, \"step\": 6850}\n",
      "{\"learning_rate\": 1.0637065637065636e-06, \"loss\": 1.515750563144684, \"step\": 6900}\n",
      "{\"learning_rate\": 1.0025740025740025e-06, \"loss\": 1.5230033683776856, \"step\": 6950}\n",
      "{'acc': 0.14313346228239845}\n",
      "{\"eval_acc\": 0.14313346228239845, \"learning_rate\": 9.5e-07, \"loss\": 1.319391543865204, \"step\": 6993}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|█████████ | 27/30 [1:16:30<08:32, 170.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 9.414414414414415e-07, \"loss\": 0.2128976106643677, \"step\": 7000}\n",
      "{\"learning_rate\": 8.803088803088804e-07, \"loss\": 1.5052004480361938, \"step\": 7050}\n",
      "{\"learning_rate\": 8.191763191763191e-07, \"loss\": 1.5178054237365723, \"step\": 7100}\n",
      "{\"learning_rate\": 7.58043758043758e-07, \"loss\": 1.512448751926422, \"step\": 7150}\n",
      "{\"learning_rate\": 6.969111969111969e-07, \"loss\": 1.5082658219337464, \"step\": 7200}\n",
      "{\"learning_rate\": 6.357786357786358e-07, \"loss\": 1.532610638141632, \"step\": 7250}\n",
      "{'acc': 0.13152804642166344}\n",
      "{\"eval_acc\": 0.13152804642166344, \"learning_rate\": 6.333333333333333e-07, \"loss\": 0.06230284214019775, \"step\": 7252}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  93%|█████████▎| 28/30 [1:19:21<05:41, 170.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.746460746460746e-07, \"loss\": 1.437644271850586, \"step\": 7300}\n",
      "{\"learning_rate\": 5.135135135135135e-07, \"loss\": 1.5017216396331787, \"step\": 7350}\n",
      "{\"learning_rate\": 4.5238095238095237e-07, \"loss\": 1.5159061646461487, \"step\": 7400}\n",
      "{\"learning_rate\": 3.9124839124839125e-07, \"loss\": 1.5060063362121583, \"step\": 7450}\n",
      "{\"learning_rate\": 3.301158301158301e-07, \"loss\": 1.5136895632743836, \"step\": 7500}\n",
      "{'acc': 0.13152804642166344}\n",
      "{\"eval_acc\": 0.13152804642166344, \"learning_rate\": 3.1666666666666667e-07, \"loss\": 0.34527135372161866, \"step\": 7511}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  97%|█████████▋| 29/30 [1:22:12<02:50, 170.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.68983268983269e-07, \"loss\": 1.1936706185340882, \"step\": 7550}\n",
      "{\"learning_rate\": 2.0785070785070785e-07, \"loss\": 1.5366275191307068, \"step\": 7600}\n",
      "{\"learning_rate\": 1.4671814671814672e-07, \"loss\": 1.4956830263137817, \"step\": 7650}\n",
      "{\"learning_rate\": 8.558558558558559e-08, \"loss\": 1.4926729345321654, \"step\": 7700}\n",
      "{\"learning_rate\": 2.4453024453024453e-08, \"loss\": 1.4791073822975158, \"step\": 7750}\n",
      "{'acc': 0.13346228239845262}\n",
      "{\"eval_acc\": 0.13346228239845262, \"learning_rate\": 0.0, \"loss\": 0.599100341796875, \"step\": 7770}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SecondaryGleason/epoch-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [1:25:03<00:00, 170.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performance {'acc': 0.2495164410058027, 'checkpoint': 'epoch-10'}\n",
      "MarginStatusNone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.4453024453024455e-07, \"loss\": 0.713255615234375, \"step\": 50}\n",
      "{\"learning_rate\": 4.890604890604891e-07, \"loss\": 0.714631187915802, \"step\": 100}\n",
      "{\"learning_rate\": 7.335907335907336e-07, \"loss\": 0.7081317055225372, \"step\": 150}\n",
      "{\"learning_rate\": 9.781209781209782e-07, \"loss\": 0.7133208835124969, \"step\": 200}\n",
      "{\"learning_rate\": 1.2226512226512227e-06, \"loss\": 0.715551745891571, \"step\": 250}\n",
      "{'acc': 0.6634429400386848}\n",
      "{\"eval_acc\": 0.6634429400386848, \"learning_rate\": 1.2666666666666667e-06, \"loss\": 0.1271163511276245, \"step\": 259}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   3%|▎         | 1/30 [02:51<1:22:48, 171.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.4671814671814672e-06, \"loss\": 0.5748478615283966, \"step\": 300}\n",
      "{\"learning_rate\": 1.7117117117117117e-06, \"loss\": 0.7081383323669433, \"step\": 350}\n",
      "{\"learning_rate\": 1.9562419562419564e-06, \"loss\": 0.6975181889533997, \"step\": 400}\n",
      "{\"learning_rate\": 2.2007722007722007e-06, \"loss\": 0.7075527918338775, \"step\": 450}\n",
      "{\"learning_rate\": 2.4453024453024454e-06, \"loss\": 0.705381668806076, \"step\": 500}\n",
      "{'acc': 0.5435203094777563}\n",
      "{\"eval_acc\": 0.5435203094777563, \"learning_rate\": 2.5333333333333334e-06, \"loss\": 0.25671539068222043, \"step\": 518}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   7%|▋         | 2/30 [05:42<1:19:51, 171.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.6898326898326897e-06, \"loss\": 0.4501733672618866, \"step\": 550}\n",
      "{\"learning_rate\": 2.9343629343629344e-06, \"loss\": 0.7067060804367066, \"step\": 600}\n",
      "{\"learning_rate\": 3.1788931788931787e-06, \"loss\": 0.7037944006919861, \"step\": 650}\n",
      "{\"learning_rate\": 3.4234234234234234e-06, \"loss\": 0.7055531191825867, \"step\": 700}\n",
      "{\"learning_rate\": 3.6679536679536677e-06, \"loss\": 0.7071482396125793, \"step\": 750}\n",
      "{'acc': 0.48936170212765956}\n",
      "{\"eval_acc\": 0.48936170212765956, \"learning_rate\": 3.8e-06, \"loss\": 0.3703801155090332, \"step\": 777}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 3/30 [08:33<1:17:04, 171.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.912483912483913e-06, \"loss\": 0.3190204834938049, \"step\": 800}\n",
      "{\"learning_rate\": 4.157014157014157e-06, \"loss\": 0.7072277426719665, \"step\": 850}\n",
      "{\"learning_rate\": 4.401544401544401e-06, \"loss\": 0.7034096920490265, \"step\": 900}\n",
      "{\"learning_rate\": 4.6460746460746465e-06, \"loss\": 0.7000951004028321, \"step\": 950}\n",
      "{\"learning_rate\": 4.890604890604891e-06, \"loss\": 0.6893311321735383, \"step\": 1000}\n",
      "{'acc': 0.437137330754352}\n",
      "{\"eval_acc\": 0.437137330754352, \"learning_rate\": 5.066666666666667e-06, \"loss\": 0.503508460521698, \"step\": 1036}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  13%|█▎        | 4/30 [11:24<1:14:12, 171.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.135135135135135e-06, \"loss\": 0.19346749782562256, \"step\": 1050}\n",
      "{\"learning_rate\": 5.379665379665379e-06, \"loss\": 0.6992552840709686, \"step\": 1100}\n",
      "{\"learning_rate\": 5.6241956241956245e-06, \"loss\": 0.6939530038833618, \"step\": 1150}\n",
      "{\"learning_rate\": 5.868725868725869e-06, \"loss\": 0.7064679038524627, \"step\": 1200}\n",
      "{\"learning_rate\": 6.113256113256113e-06, \"loss\": 0.6943847513198853, \"step\": 1250}\n",
      "{'acc': 0.48936170212765956}\n",
      "{\"eval_acc\": 0.48936170212765956, \"learning_rate\": 6.333333333333333e-06, \"loss\": 0.6297370231151581, \"step\": 1295}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  17%|█▋        | 5/30 [14:16<1:11:19, 171.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.357786357786357e-06, \"loss\": 0.07199171900749207, \"step\": 1300}\n",
      "{\"learning_rate\": 6.6023166023166025e-06, \"loss\": 0.696348842382431, \"step\": 1350}\n",
      "{\"learning_rate\": 6.846846846846847e-06, \"loss\": 0.699311089515686, \"step\": 1400}\n",
      "{\"learning_rate\": 7.091377091377091e-06, \"loss\": 0.6979342067241668, \"step\": 1450}\n",
      "{\"learning_rate\": 7.335907335907335e-06, \"loss\": 0.6957134246826172, \"step\": 1500}\n",
      "{\"learning_rate\": 7.5804375804375804e-06, \"loss\": 0.6954893612861633, \"step\": 1550}\n",
      "{'acc': 0.4197292069632495}\n",
      "{\"eval_acc\": 0.4197292069632495, \"learning_rate\": 7.6e-06, \"loss\": 0.05462263226509094, \"step\": 1554}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 6/30 [17:07<1:08:26, 171.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 7.543758043758044e-06, \"loss\": 0.6257934474945068, \"step\": 1600}\n",
      "{\"learning_rate\": 7.482625482625483e-06, \"loss\": 0.7001949965953826, \"step\": 1650}\n",
      "{\"learning_rate\": 7.4214929214929216e-06, \"loss\": 0.7015164995193481, \"step\": 1700}\n",
      "{\"learning_rate\": 7.36036036036036e-06, \"loss\": 0.6830526292324066, \"step\": 1750}\n",
      "{\"learning_rate\": 7.2992277992277994e-06, \"loss\": 0.6914076697826386, \"step\": 1800}\n",
      "{'acc': 0.6266924564796905}\n",
      "{\"eval_acc\": 0.6266924564796905, \"learning_rate\": 7.283333333333334e-06, \"loss\": 0.18537998914718629, \"step\": 1813}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  23%|██▎       | 7/30 [19:58<1:05:38, 171.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 7.238095238095238e-06, \"loss\": 0.5252608847618103, \"step\": 1850}\n",
      "{\"learning_rate\": 7.176962676962677e-06, \"loss\": 0.6948099327087403, \"step\": 1900}\n",
      "{\"learning_rate\": 7.115830115830116e-06, \"loss\": 0.6879335427284241, \"step\": 1950}\n",
      "{\"learning_rate\": 7.054697554697555e-06, \"loss\": 0.6944618654251099, \"step\": 2000}\n",
      "{\"learning_rate\": 6.993564993564994e-06, \"loss\": 0.691185474395752, \"step\": 2050}\n",
      "{'acc': 0.5203094777562862}\n",
      "{\"eval_acc\": 0.5203094777562862, \"learning_rate\": 6.966666666666666e-06, \"loss\": 0.3062968635559082, \"step\": 2072}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  27%|██▋       | 8/30 [22:49<1:02:44, 171.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.932432432432432e-06, \"loss\": 0.3967524993419647, \"step\": 2100}\n",
      "{\"learning_rate\": 6.8712998712998715e-06, \"loss\": 0.6977728343009949, \"step\": 2150}\n",
      "{\"learning_rate\": 6.81016731016731e-06, \"loss\": 0.6859904277324677, \"step\": 2200}\n",
      "{\"learning_rate\": 6.749034749034749e-06, \"loss\": 0.6941677808761597, \"step\": 2250}\n",
      "{\"learning_rate\": 6.687902187902188e-06, \"loss\": 0.6939329290390015, \"step\": 2300}\n",
      "{'acc': 0.4758220502901354}\n",
      "{\"eval_acc\": 0.4758220502901354, \"learning_rate\": 6.65e-06, \"loss\": 0.4229567629098892, \"step\": 2331}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 9/30 [25:40<59:53, 171.13s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.626769626769626e-06, \"loss\": 0.25992774724960327, \"step\": 2350}\n",
      "{\"learning_rate\": 6.565637065637066e-06, \"loss\": 0.6863111746311188, \"step\": 2400}\n",
      "{\"learning_rate\": 6.504504504504505e-06, \"loss\": 0.7015438663959503, \"step\": 2450}\n",
      "{\"learning_rate\": 6.443371943371943e-06, \"loss\": 0.6939216887950898, \"step\": 2500}\n",
      "{\"learning_rate\": 6.382239382239382e-06, \"loss\": 0.6841826176643372, \"step\": 2550}\n",
      "{'acc': 0.528046421663443}\n",
      "{\"eval_acc\": 0.528046421663443, \"learning_rate\": 6.333333333333333e-06, \"loss\": 0.5552013087272644, \"step\": 2590}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███▎      | 10/30 [28:31<56:58, 170.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.3211068211068214e-06, \"loss\": 0.13651878833770753, \"step\": 2600}\n",
      "{\"learning_rate\": 6.25997425997426e-06, \"loss\": 0.6865573132038116, \"step\": 2650}\n",
      "{\"learning_rate\": 6.1988416988416985e-06, \"loss\": 0.6925894105434418, \"step\": 2700}\n",
      "{\"learning_rate\": 6.137709137709138e-06, \"loss\": 0.6937070429325104, \"step\": 2750}\n",
      "{\"learning_rate\": 6.076576576576577e-06, \"loss\": 0.6857493233680725, \"step\": 2800}\n",
      "{'acc': 0.6266924564796905}\n",
      "{\"eval_acc\": 0.6266924564796905, \"learning_rate\": 6.016666666666666e-06, \"loss\": 0.6728388261795044, \"step\": 2849}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  37%|███▋      | 11/30 [31:21<54:05, 170.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.015444015444016e-06, \"loss\": 0.013639408349990844, \"step\": 2850}\n",
      "{\"learning_rate\": 5.954311454311454e-06, \"loss\": 0.6927215874195098, \"step\": 2900}\n",
      "{\"learning_rate\": 5.8931788931788935e-06, \"loss\": 0.7028904604911804, \"step\": 2950}\n",
      "{\"learning_rate\": 5.832046332046332e-06, \"loss\": 0.6956779551506043, \"step\": 3000}\n",
      "{\"learning_rate\": 5.770913770913771e-06, \"loss\": 0.6991350698471069, \"step\": 3050}\n",
      "{\"learning_rate\": 5.70978120978121e-06, \"loss\": 0.7004632246494293, \"step\": 3100}\n",
      "{'acc': 0.5899419729206963}\n",
      "{\"eval_acc\": 0.5899419729206963, \"learning_rate\": 5.7e-06, \"loss\": 0.11393651604652405, \"step\": 3108}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 12/30 [34:12<51:14, 170.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.648648648648648e-06, \"loss\": 0.5808119201660156, \"step\": 3150}\n",
      "{\"learning_rate\": 5.587516087516088e-06, \"loss\": 0.6926499104499817, \"step\": 3200}\n",
      "{\"learning_rate\": 5.526383526383526e-06, \"loss\": 0.6827987945079803, \"step\": 3250}\n",
      "{\"learning_rate\": 5.465250965250965e-06, \"loss\": 0.6820221543312073, \"step\": 3300}\n",
      "{\"learning_rate\": 5.404118404118404e-06, \"loss\": 0.6841409170627594, \"step\": 3350}\n",
      "{'acc': 0.5454545454545454}\n",
      "{\"eval_acc\": 0.5454545454545454, \"learning_rate\": 5.383333333333334e-06, \"loss\": 0.23406134247779847, \"step\": 3367}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  43%|████▎     | 13/30 [37:03<48:23, 170.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.3429858429858435e-06, \"loss\": 0.4519999361038208, \"step\": 3400}\n",
      "{\"learning_rate\": 5.281853281853282e-06, \"loss\": 0.6864813661575317, \"step\": 3450}\n",
      "{\"learning_rate\": 5.2207207207207205e-06, \"loss\": 0.6900097405910492, \"step\": 3500}\n",
      "{\"learning_rate\": 5.15958815958816e-06, \"loss\": 0.6784766602516175, \"step\": 3550}\n",
      "{\"learning_rate\": 5.098455598455599e-06, \"loss\": 0.6753108024597168, \"step\": 3600}\n",
      "{'acc': 0.5338491295938105}\n",
      "{\"eval_acc\": 0.5338491295938105, \"learning_rate\": 5.066666666666667e-06, \"loss\": 0.3554686439037323, \"step\": 3626}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  47%|████▋     | 14/30 [39:53<45:30, 170.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.037323037323037e-06, \"loss\": 0.32926052808761597, \"step\": 3650}\n",
      "{\"learning_rate\": 4.976190476190476e-06, \"loss\": 0.6815730702877044, \"step\": 3700}\n",
      "{\"learning_rate\": 4.9150579150579155e-06, \"loss\": 0.6899321162700653, \"step\": 3750}\n",
      "{\"learning_rate\": 4.853925353925354e-06, \"loss\": 0.6805051219463348, \"step\": 3800}\n",
      "{\"learning_rate\": 4.7927927927927925e-06, \"loss\": 0.6822946250438691, \"step\": 3850}\n",
      "{'acc': 0.574468085106383}\n",
      "{\"eval_acc\": 0.574468085106383, \"learning_rate\": 4.75e-06, \"loss\": 0.46854453563690185, \"step\": 3885}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 15/30 [42:43<42:39, 170.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 4.731660231660232e-06, \"loss\": 0.2040345060825348, \"step\": 3900}\n",
      "{\"learning_rate\": 4.67052767052767e-06, \"loss\": 0.6886547613143921, \"step\": 3950}\n",
      "{\"learning_rate\": 4.60939510939511e-06, \"loss\": 0.6763736569881439, \"step\": 4000}\n",
      "{\"learning_rate\": 4.548262548262548e-06, \"loss\": 0.6774506485462188, \"step\": 4050}\n",
      "{\"learning_rate\": 4.487129987129987e-06, \"loss\": 0.6902784836292267, \"step\": 4100}\n",
      "{'acc': 0.5589941972920697}\n",
      "{\"eval_acc\": 0.5589941972920697, \"learning_rate\": 4.433333333333334e-06, \"loss\": 0.596810702085495, \"step\": 4144}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  53%|█████▎    | 16/30 [45:34<39:48, 170.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 4.425997425997426e-06, \"loss\": 0.08176504373550415, \"step\": 4150}\n",
      "{\"learning_rate\": 4.364864864864865e-06, \"loss\": 0.6915999603271484, \"step\": 4200}\n",
      "{\"learning_rate\": 4.303732303732304e-06, \"loss\": 0.6816961812973023, \"step\": 4250}\n",
      "{\"learning_rate\": 4.2425997425997425e-06, \"loss\": 0.6741625618934631, \"step\": 4300}\n",
      "{\"learning_rate\": 4.181467181467182e-06, \"loss\": 0.674964085817337, \"step\": 4350}\n",
      "{\"learning_rate\": 4.12033462033462e-06, \"loss\": 0.6821423971652985, \"step\": 4400}\n",
      "{'acc': 0.5957446808510638}\n",
      "{\"eval_acc\": 0.5957446808510638, \"learning_rate\": 4.1166666666666665e-06, \"loss\": 0.03809941053390503, \"step\": 4403}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  57%|█████▋    | 17/30 [48:25<36:58, 170.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 4.059202059202059e-06, \"loss\": 0.648282128572464, \"step\": 4450}\n",
      "{\"learning_rate\": 3.998069498069498e-06, \"loss\": 0.6774367415904998, \"step\": 4500}\n",
      "{\"learning_rate\": 3.9369369369369376e-06, \"loss\": 0.6801886284351348, \"step\": 4550}\n",
      "{\"learning_rate\": 3.875804375804375e-06, \"loss\": 0.6843801355361938, \"step\": 4600}\n",
      "{\"learning_rate\": 3.8146718146718146e-06, \"loss\": 0.6951964616775512, \"step\": 4650}\n",
      "{'acc': 0.5299806576402321}\n",
      "{\"eval_acc\": 0.5299806576402321, \"learning_rate\": 3.8e-06, \"loss\": 0.16608080625534058, \"step\": 4662}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 18/30 [51:16<34:08, 170.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.7535392535392535e-06, \"loss\": 0.5171662449836731, \"step\": 4700}\n",
      "{\"learning_rate\": 3.6924066924066924e-06, \"loss\": 0.6777529621124267, \"step\": 4750}\n",
      "{\"learning_rate\": 3.6312741312741313e-06, \"loss\": 0.6909279119968414, \"step\": 4800}\n",
      "{\"learning_rate\": 3.5701415701415703e-06, \"loss\": 0.6709759378433228, \"step\": 4850}\n",
      "{\"learning_rate\": 3.509009009009009e-06, \"loss\": 0.6931202495098114, \"step\": 4900}\n",
      "{'acc': 0.5319148936170213}\n",
      "{\"eval_acc\": 0.5319148936170213, \"learning_rate\": 3.483333333333333e-06, \"loss\": 0.2830205190181732, \"step\": 4921}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  63%|██████▎   | 19/30 [54:06<31:18, 170.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.4478764478764477e-06, \"loss\": 0.39959486842155456, \"step\": 4950}\n",
      "{\"learning_rate\": 3.386743886743887e-06, \"loss\": 0.6750699377059937, \"step\": 5000}\n",
      "{\"learning_rate\": 3.3256113256113256e-06, \"loss\": 0.6827454626560211, \"step\": 5050}\n",
      "{\"learning_rate\": 3.2644787644787645e-06, \"loss\": 0.7051391232013703, \"step\": 5100}\n",
      "{\"learning_rate\": 3.2033462033462034e-06, \"loss\": 0.68675567984581, \"step\": 5150}\n",
      "{'acc': 0.5454545454545454}\n",
      "{\"eval_acc\": 0.5454545454545454, \"learning_rate\": 3.1666666666666667e-06, \"loss\": 0.4177081489562988, \"step\": 5180}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████▋   | 20/30 [56:57<28:27, 170.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.1422136422136424e-06, \"loss\": 0.28036064743995665, \"step\": 5200}\n",
      "{\"learning_rate\": 3.0810810810810813e-06, \"loss\": 0.6750571811199189, \"step\": 5250}\n",
      "{\"learning_rate\": 3.01994851994852e-06, \"loss\": 0.7013505983352661, \"step\": 5300}\n",
      "{\"learning_rate\": 2.9588159588159587e-06, \"loss\": 0.6821556401252746, \"step\": 5350}\n",
      "{\"learning_rate\": 2.8976833976833976e-06, \"loss\": 0.6886204552650451, \"step\": 5400}\n",
      "{'acc': 0.5570599613152805}\n",
      "{\"eval_acc\": 0.5570599613152805, \"learning_rate\": 2.85e-06, \"loss\": 0.529189088344574, \"step\": 5439}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|███████   | 21/30 [59:48<25:36, 170.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.8365508365508366e-06, \"loss\": 0.14895930171012878, \"step\": 5450}\n",
      "{\"learning_rate\": 2.775418275418275e-06, \"loss\": 0.6729971945285798, \"step\": 5500}\n",
      "{\"learning_rate\": 2.7142857142857144e-06, \"loss\": 0.6853962552547455, \"step\": 5550}\n",
      "{\"learning_rate\": 2.653153153153153e-06, \"loss\": 0.6801918947696686, \"step\": 5600}\n",
      "{\"learning_rate\": 2.5920205920205923e-06, \"loss\": 0.6828604054450989, \"step\": 5650}\n",
      "{'acc': 0.5570599613152805}\n",
      "{\"eval_acc\": 0.5570599613152805, \"learning_rate\": 2.5333333333333334e-06, \"loss\": 0.6548496878147125, \"step\": 5698}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  73%|███████▎  | 22/30 [1:02:38<22:45, 170.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.530888030888031e-06, \"loss\": 0.025756906270980835, \"step\": 5700}\n",
      "{\"learning_rate\": 2.4697554697554697e-06, \"loss\": 0.6819223439693451, \"step\": 5750}\n",
      "{\"learning_rate\": 2.4086229086229087e-06, \"loss\": 0.6756268393993378, \"step\": 5800}\n",
      "{\"learning_rate\": 2.3474903474903476e-06, \"loss\": 0.68523193359375, \"step\": 5850}\n",
      "{\"learning_rate\": 2.286357786357786e-06, \"loss\": 0.69742192029953, \"step\": 5900}\n",
      "{\"learning_rate\": 2.2252252252252254e-06, \"loss\": 0.6809181118011475, \"step\": 5950}\n",
      "{'acc': 0.5705996131528046}\n",
      "{\"eval_acc\": 0.5705996131528046, \"learning_rate\": 2.216666666666667e-06, \"loss\": 0.09311842560768127, \"step\": 5957}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  77%|███████▋  | 23/30 [1:05:29<19:54, 170.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.164092664092664e-06, \"loss\": 0.576875205039978, \"step\": 6000}\n",
      "{\"learning_rate\": 2.1029601029601033e-06, \"loss\": 0.686265013217926, \"step\": 6050}\n",
      "{\"learning_rate\": 2.041827541827542e-06, \"loss\": 0.6757192420959472, \"step\": 6100}\n",
      "{\"learning_rate\": 1.9806949806949807e-06, \"loss\": 0.6783711171150207, \"step\": 6150}\n",
      "{\"learning_rate\": 1.9195624195624197e-06, \"loss\": 0.6808604276180268, \"step\": 6200}\n",
      "{'acc': 0.5725338491295938}\n",
      "{\"eval_acc\": 0.5725338491295938, \"learning_rate\": 1.9e-06, \"loss\": 0.22163416624069213, \"step\": 6216}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 24/30 [1:08:20<17:03, 170.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.8584298584298584e-06, \"loss\": 0.45666232347488406, \"step\": 6250}\n",
      "{\"learning_rate\": 1.7972972972972973e-06, \"loss\": 0.6824343478679658, \"step\": 6300}\n",
      "{\"learning_rate\": 1.7361647361647362e-06, \"loss\": 0.6744835186004638, \"step\": 6350}\n",
      "{\"learning_rate\": 1.6750321750321752e-06, \"loss\": 0.6806786060333252, \"step\": 6400}\n",
      "{\"learning_rate\": 1.6138996138996139e-06, \"loss\": 0.6745041000843048, \"step\": 6450}\n",
      "{'acc': 0.5725338491295938}\n",
      "{\"eval_acc\": 0.5725338491295938, \"learning_rate\": 1.5833333333333333e-06, \"loss\": 0.33360799908638, \"step\": 6475}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  83%|████████▎ | 25/30 [1:11:10<14:13, 170.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.5527670527670528e-06, \"loss\": 0.3334659206867218, \"step\": 6500}\n",
      "{\"learning_rate\": 1.4916344916344917e-06, \"loss\": 0.674998596906662, \"step\": 6550}\n",
      "{\"learning_rate\": 1.4305019305019307e-06, \"loss\": 0.6663364958763123, \"step\": 6600}\n",
      "{\"learning_rate\": 1.3693693693693694e-06, \"loss\": 0.6711671030521393, \"step\": 6650}\n",
      "{\"learning_rate\": 1.3082368082368083e-06, \"loss\": 0.6722612977027893, \"step\": 6700}\n",
      "{'acc': 0.562862669245648}\n",
      "{\"eval_acc\": 0.562862669245648, \"learning_rate\": 1.2666666666666667e-06, \"loss\": 0.464124892950058, \"step\": 6734}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  87%|████████▋ | 26/30 [1:14:01<11:22, 170.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.2471042471042472e-06, \"loss\": 0.2203032386302948, \"step\": 6750}\n",
      "{\"learning_rate\": 1.1859716859716862e-06, \"loss\": 0.6864728474617005, \"step\": 6800}\n",
      "{\"learning_rate\": 1.1248391248391247e-06, \"loss\": 0.6813849771022796, \"step\": 6850}\n",
      "{\"learning_rate\": 1.0637065637065636e-06, \"loss\": 0.6780276811122894, \"step\": 6900}\n",
      "{\"learning_rate\": 1.0025740025740025e-06, \"loss\": 0.671302330493927, \"step\": 6950}\n",
      "{'acc': 0.5609284332688588}\n",
      "{\"eval_acc\": 0.5609284332688588, \"learning_rate\": 9.5e-07, \"loss\": 0.584727908372879, \"step\": 6993}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|█████████ | 27/30 [1:16:51<08:31, 170.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 9.414414414414415e-07, \"loss\": 0.09687476754188537, \"step\": 7000}\n",
      "{\"learning_rate\": 8.803088803088804e-07, \"loss\": 0.6881019127368927, \"step\": 7050}\n",
      "{\"learning_rate\": 8.191763191763191e-07, \"loss\": 0.6724404954910278, \"step\": 7100}\n",
      "{\"learning_rate\": 7.58043758043758e-07, \"loss\": 0.6827603876590729, \"step\": 7150}\n",
      "{\"learning_rate\": 6.969111969111969e-07, \"loss\": 0.6771045625209808, \"step\": 7200}\n",
      "{\"learning_rate\": 6.357786357786358e-07, \"loss\": 0.6870536494255066, \"step\": 7250}\n",
      "{'acc': 0.562862669245648}\n",
      "{\"eval_acc\": 0.562862669245648, \"learning_rate\": 6.333333333333333e-07, \"loss\": 0.030141568183898924, \"step\": 7252}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  93%|█████████▎| 28/30 [1:19:42<05:41, 170.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.746460746460746e-07, \"loss\": 0.6454072773456574, \"step\": 7300}\n",
      "{\"learning_rate\": 5.135135135135135e-07, \"loss\": 0.6785387289524079, \"step\": 7350}\n",
      "{\"learning_rate\": 4.5238095238095237e-07, \"loss\": 0.6926439189910889, \"step\": 7400}\n",
      "{\"learning_rate\": 3.9124839124839125e-07, \"loss\": 0.6699007308483124, \"step\": 7450}\n",
      "{\"learning_rate\": 3.301158301158301e-07, \"loss\": 0.6587452554702758, \"step\": 7500}\n",
      "{'acc': 0.5609284332688588}\n",
      "{\"eval_acc\": 0.5609284332688588, \"learning_rate\": 3.1666666666666667e-07, \"loss\": 0.1473517394065857, \"step\": 7511}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  97%|█████████▋| 29/30 [1:22:33<02:50, 170.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.68983268983269e-07, \"loss\": 0.5217256844043732, \"step\": 7550}\n",
      "{\"learning_rate\": 2.0785070785070785e-07, \"loss\": 0.6862090504169465, \"step\": 7600}\n",
      "{\"learning_rate\": 1.4671814671814672e-07, \"loss\": 0.6803025615215301, \"step\": 7650}\n",
      "{\"learning_rate\": 8.558558558558559e-08, \"loss\": 0.6846035587787628, \"step\": 7700}\n",
      "{\"learning_rate\": 2.4453024453024453e-08, \"loss\": 0.6773081517219544, \"step\": 7750}\n",
      "{'acc': 0.562862669245648}\n",
      "{\"eval_acc\": 0.562862669245648, \"learning_rate\": 0.0, \"loss\": 0.2789958190917969, \"step\": 7770}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/MarginStatusNone/epoch-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [1:25:24<00:00, 170.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performance {'acc': 0.6634429400386848, 'checkpoint': 'epoch-1'}\n",
      "SeminalVesicleNone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.4453024453024455e-07, \"loss\": 0.6924973547458648, \"step\": 50}\n",
      "{\"learning_rate\": 4.890604890604891e-07, \"loss\": 0.686991970539093, \"step\": 100}\n",
      "{\"learning_rate\": 7.335907335907336e-07, \"loss\": 0.6985612571239471, \"step\": 150}\n",
      "{\"learning_rate\": 9.781209781209782e-07, \"loss\": 0.7002188265323639, \"step\": 200}\n",
      "{\"learning_rate\": 1.2226512226512227e-06, \"loss\": 0.6981425428390503, \"step\": 250}\n",
      "{'acc': 0.7021276595744681}\n",
      "{\"eval_acc\": 0.7021276595744681, \"learning_rate\": 1.2666666666666667e-06, \"loss\": 0.1268031430244446, \"step\": 259}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   3%|▎         | 1/30 [02:50<1:22:28, 170.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.4671814671814672e-06, \"loss\": 0.5719889175891876, \"step\": 300}\n",
      "{\"learning_rate\": 1.7117117117117117e-06, \"loss\": 0.7019189631938935, \"step\": 350}\n",
      "{\"learning_rate\": 1.9562419562419564e-06, \"loss\": 0.7028010523319245, \"step\": 400}\n",
      "{\"learning_rate\": 2.2007722007722007e-06, \"loss\": 0.6906898212432862, \"step\": 450}\n",
      "{\"learning_rate\": 2.4453024453024454e-06, \"loss\": 0.6926755201816559, \"step\": 500}\n",
      "{'acc': 0.6518375241779497}\n",
      "{\"eval_acc\": 0.6518375241779497, \"learning_rate\": 2.5333333333333334e-06, \"loss\": 0.25808226108551025, \"step\": 518}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   7%|▋         | 2/30 [05:41<1:19:38, 170.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.6898326898326897e-06, \"loss\": 0.44654912829399107, \"step\": 550}\n",
      "{\"learning_rate\": 2.9343629343629344e-06, \"loss\": 0.6997661113739013, \"step\": 600}\n",
      "{\"learning_rate\": 3.1788931788931787e-06, \"loss\": 0.7062646603584289, \"step\": 650}\n",
      "{\"learning_rate\": 3.4234234234234234e-06, \"loss\": 0.6973188245296478, \"step\": 700}\n",
      "{\"learning_rate\": 3.6679536679536677e-06, \"loss\": 0.6985662424564362, \"step\": 750}\n",
      "{'acc': 0.4835589941972921}\n",
      "{\"eval_acc\": 0.4835589941972921, \"learning_rate\": 3.8e-06, \"loss\": 0.37238973736763, \"step\": 777}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 3/30 [08:34<1:17:18, 171.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.912483912483913e-06, \"loss\": 0.32066099643707274, \"step\": 800}\n",
      "{\"learning_rate\": 4.157014157014157e-06, \"loss\": 0.7055043411254883, \"step\": 850}\n",
      "{\"learning_rate\": 4.401544401544401e-06, \"loss\": 0.7032226073741913, \"step\": 900}\n",
      "{\"learning_rate\": 4.6460746460746465e-06, \"loss\": 0.7051781749725342, \"step\": 950}\n",
      "{\"learning_rate\": 4.890604890604891e-06, \"loss\": 0.6962650525569916, \"step\": 1000}\n",
      "{'acc': 0.3481624758220503}\n",
      "{\"eval_acc\": 0.3481624758220503, \"learning_rate\": 5.066666666666667e-06, \"loss\": 0.5166411030292511, \"step\": 1036}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  13%|█▎        | 4/30 [11:25<1:14:16, 171.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.135135135135135e-06, \"loss\": 0.18961531519889832, \"step\": 1050}\n",
      "{\"learning_rate\": 5.379665379665379e-06, \"loss\": 0.6975899374485016, \"step\": 1100}\n",
      "{\"learning_rate\": 5.6241956241956245e-06, \"loss\": 0.7012627375125885, \"step\": 1150}\n",
      "{\"learning_rate\": 5.868725868725869e-06, \"loss\": 0.6918975186347961, \"step\": 1200}\n",
      "{\"learning_rate\": 6.113256113256113e-06, \"loss\": 0.6951007688045502, \"step\": 1250}\n",
      "{'acc': 0.4197292069632495}\n",
      "{\"eval_acc\": 0.4197292069632495, \"learning_rate\": 6.333333333333333e-06, \"loss\": 0.6281949937343597, \"step\": 1295}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  17%|█▋        | 5/30 [14:15<1:11:18, 171.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.357786357786357e-06, \"loss\": 0.06785924196243286, \"step\": 1300}\n",
      "{\"learning_rate\": 6.6023166023166025e-06, \"loss\": 0.6904037606716156, \"step\": 1350}\n",
      "{\"learning_rate\": 6.846846846846847e-06, \"loss\": 0.7048596513271331, \"step\": 1400}\n",
      "{\"learning_rate\": 7.091377091377091e-06, \"loss\": 0.6902806603908539, \"step\": 1450}\n",
      "{\"learning_rate\": 7.335907335907335e-06, \"loss\": 0.6942231404781342, \"step\": 1500}\n",
      "{\"learning_rate\": 7.5804375804375804e-06, \"loss\": 0.690731555223465, \"step\": 1550}\n",
      "{'acc': 0.5357833655705996}\n",
      "{\"eval_acc\": 0.5357833655705996, \"learning_rate\": 7.6e-06, \"loss\": 0.05729901075363159, \"step\": 1554}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 6/30 [17:06<1:08:24, 171.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 7.543758043758044e-06, \"loss\": 0.636052668094635, \"step\": 1600}\n",
      "{\"learning_rate\": 7.482625482625483e-06, \"loss\": 0.6864431393146515, \"step\": 1650}\n",
      "{\"learning_rate\": 7.4214929214929216e-06, \"loss\": 0.6901387453079224, \"step\": 1700}\n",
      "{\"learning_rate\": 7.36036036036036e-06, \"loss\": 0.6918410336971283, \"step\": 1750}\n",
      "{\"learning_rate\": 7.2992277992277994e-06, \"loss\": 0.6906998860836029, \"step\": 1800}\n",
      "{'acc': 0.42940038684719534}\n",
      "{\"eval_acc\": 0.42940038684719534, \"learning_rate\": 7.283333333333334e-06, \"loss\": 0.18062324285507203, \"step\": 1813}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  23%|██▎       | 7/30 [19:57<1:05:29, 170.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 7.238095238095238e-06, \"loss\": 0.512027850151062, \"step\": 1850}\n",
      "{\"learning_rate\": 7.176962676962677e-06, \"loss\": 0.6894539737701416, \"step\": 1900}\n",
      "{\"learning_rate\": 7.115830115830116e-06, \"loss\": 0.6879581701755524, \"step\": 1950}\n",
      "{\"learning_rate\": 7.054697554697555e-06, \"loss\": 0.6883527660369873, \"step\": 2000}\n",
      "{\"learning_rate\": 6.993564993564994e-06, \"loss\": 0.683466340303421, \"step\": 2050}\n",
      "{'acc': 0.4564796905222437}\n",
      "{\"eval_acc\": 0.4564796905222437, \"learning_rate\": 6.966666666666666e-06, \"loss\": 0.3126652419567108, \"step\": 2072}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  27%|██▋       | 8/30 [22:47<1:02:36, 170.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.932432432432432e-06, \"loss\": 0.37624908685684205, \"step\": 2100}\n",
      "{\"learning_rate\": 6.8712998712998715e-06, \"loss\": 0.6934744453430176, \"step\": 2150}\n",
      "{\"learning_rate\": 6.81016731016731e-06, \"loss\": 0.6780937492847443, \"step\": 2200}\n",
      "{\"learning_rate\": 6.749034749034749e-06, \"loss\": 0.6770160508155822, \"step\": 2250}\n",
      "{\"learning_rate\": 6.687902187902188e-06, \"loss\": 0.6928148555755616, \"step\": 2300}\n",
      "{'acc': 0.4622823984526112}\n",
      "{\"eval_acc\": 0.4622823984526112, \"learning_rate\": 6.65e-06, \"loss\": 0.43276363849639893, \"step\": 2331}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 9/30 [25:38<59:45, 170.73s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.626769626769626e-06, \"loss\": 0.2639087641239166, \"step\": 2350}\n",
      "{\"learning_rate\": 6.565637065637066e-06, \"loss\": 0.6883484673500061, \"step\": 2400}\n",
      "{\"learning_rate\": 6.504504504504505e-06, \"loss\": 0.6861591637134552, \"step\": 2450}\n",
      "{\"learning_rate\": 6.443371943371943e-06, \"loss\": 0.6847540032863617, \"step\": 2500}\n",
      "{\"learning_rate\": 6.382239382239382e-06, \"loss\": 0.6760618233680725, \"step\": 2550}\n",
      "{'acc': 0.4158607350096712}\n",
      "{\"eval_acc\": 0.4158607350096712, \"learning_rate\": 6.333333333333333e-06, \"loss\": 0.5462145870923996, \"step\": 2590}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███▎      | 10/30 [28:29<56:54, 170.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.3211068211068214e-06, \"loss\": 0.13818953990936278, \"step\": 2600}\n",
      "{\"learning_rate\": 6.25997425997426e-06, \"loss\": 0.6850338280200958, \"step\": 2650}\n",
      "{\"learning_rate\": 6.1988416988416985e-06, \"loss\": 0.686951984167099, \"step\": 2700}\n",
      "{\"learning_rate\": 6.137709137709138e-06, \"loss\": 0.694205275774002, \"step\": 2750}\n",
      "{\"learning_rate\": 6.076576576576577e-06, \"loss\": 0.6828950226306916, \"step\": 2800}\n",
      "{'acc': 0.4816247582205029}\n",
      "{\"eval_acc\": 0.4816247582205029, \"learning_rate\": 6.016666666666666e-06, \"loss\": 0.6734871459007263, \"step\": 2849}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  37%|███▋      | 11/30 [31:19<54:03, 170.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.015444015444016e-06, \"loss\": 0.012888516187667847, \"step\": 2850}\n",
      "{\"learning_rate\": 5.954311454311454e-06, \"loss\": 0.6798894929885865, \"step\": 2900}\n",
      "{\"learning_rate\": 5.8931788931788935e-06, \"loss\": 0.6852159440517426, \"step\": 2950}\n",
      "{\"learning_rate\": 5.832046332046332e-06, \"loss\": 0.6946537494659424, \"step\": 3000}\n",
      "{\"learning_rate\": 5.770913770913771e-06, \"loss\": 0.6915663087368011, \"step\": 3050}\n",
      "{\"learning_rate\": 5.70978120978121e-06, \"loss\": 0.6856152093410492, \"step\": 3100}\n",
      "{'acc': 0.504835589941973}\n",
      "{\"eval_acc\": 0.504835589941973, \"learning_rate\": 5.7e-06, \"loss\": 0.1076326048374176, \"step\": 3108}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 12/30 [34:10<51:11, 170.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.648648648648648e-06, \"loss\": 0.5788927042484283, \"step\": 3150}\n",
      "{\"learning_rate\": 5.587516087516088e-06, \"loss\": 0.6759828913211823, \"step\": 3200}\n",
      "{\"learning_rate\": 5.526383526383526e-06, \"loss\": 0.6849043440818786, \"step\": 3250}\n",
      "{\"learning_rate\": 5.465250965250965e-06, \"loss\": 0.6773302376270294, \"step\": 3300}\n",
      "{\"learning_rate\": 5.404118404118404e-06, \"loss\": 0.6811048519611359, \"step\": 3350}\n",
      "{'acc': 0.47775628626692457}\n",
      "{\"eval_acc\": 0.47775628626692457, \"learning_rate\": 5.383333333333334e-06, \"loss\": 0.23714727640151978, \"step\": 3367}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  43%|████▎     | 13/30 [37:01<48:21, 170.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.3429858429858435e-06, \"loss\": 0.4468117642402649, \"step\": 3400}\n",
      "{\"learning_rate\": 5.281853281853282e-06, \"loss\": 0.6813707673549652, \"step\": 3450}\n",
      "{\"learning_rate\": 5.2207207207207205e-06, \"loss\": 0.6971998345851899, \"step\": 3500}\n",
      "{\"learning_rate\": 5.15958815958816e-06, \"loss\": 0.6761290538311004, \"step\": 3550}\n",
      "{\"learning_rate\": 5.098455598455599e-06, \"loss\": 0.6873891961574554, \"step\": 3600}\n",
      "{'acc': 0.5667311411992263}\n",
      "{\"eval_acc\": 0.5667311411992263, \"learning_rate\": 5.066666666666667e-06, \"loss\": 0.3472944736480713, \"step\": 3626}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  47%|████▋     | 14/30 [39:51<45:30, 170.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.037323037323037e-06, \"loss\": 0.31953946471214295, \"step\": 3650}\n",
      "{\"learning_rate\": 4.976190476190476e-06, \"loss\": 0.6868235313892365, \"step\": 3700}\n",
      "{\"learning_rate\": 4.9150579150579155e-06, \"loss\": 0.6784378206729889, \"step\": 3750}\n",
      "{\"learning_rate\": 4.853925353925354e-06, \"loss\": 0.6714998054504394, \"step\": 3800}\n",
      "{\"learning_rate\": 4.7927927927927925e-06, \"loss\": 0.6818400800228119, \"step\": 3850}\n",
      "{'acc': 0.6054158607350096}\n",
      "{\"eval_acc\": 0.6054158607350096, \"learning_rate\": 4.75e-06, \"loss\": 0.4677107059955597, \"step\": 3885}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 15/30 [42:42<42:40, 170.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 4.731660231660232e-06, \"loss\": 0.2032235884666443, \"step\": 3900}\n",
      "{\"learning_rate\": 4.67052767052767e-06, \"loss\": 0.6899878752231597, \"step\": 3950}\n",
      "{\"learning_rate\": 4.60939510939511e-06, \"loss\": 0.6877671456336976, \"step\": 4000}\n",
      "{\"learning_rate\": 4.548262548262548e-06, \"loss\": 0.6739947485923767, \"step\": 4050}\n",
      "{\"learning_rate\": 4.487129987129987e-06, \"loss\": 0.6860191559791565, \"step\": 4100}\n",
      "{'acc': 0.5822050290135397}\n",
      "{\"eval_acc\": 0.5822050290135397, \"learning_rate\": 4.433333333333334e-06, \"loss\": 0.5972157502174378, \"step\": 4144}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  53%|█████▎    | 16/30 [45:33<39:49, 170.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 4.425997425997426e-06, \"loss\": 0.08436897397041321, \"step\": 4150}\n",
      "{\"learning_rate\": 4.364864864864865e-06, \"loss\": 0.6911038720607757, \"step\": 4200}\n",
      "{\"learning_rate\": 4.303732303732304e-06, \"loss\": 0.6893789660930634, \"step\": 4250}\n",
      "{\"learning_rate\": 4.2425997425997425e-06, \"loss\": 0.6689426016807556, \"step\": 4300}\n",
      "{\"learning_rate\": 4.181467181467182e-06, \"loss\": 0.6791734373569489, \"step\": 4350}\n",
      "{\"learning_rate\": 4.12033462033462e-06, \"loss\": 0.6932197475433349, \"step\": 4400}\n",
      "{'acc': 0.5647969052224371}\n",
      "{\"eval_acc\": 0.5647969052224371, \"learning_rate\": 4.1166666666666665e-06, \"loss\": 0.04198766589164734, \"step\": 4403}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  57%|█████▋    | 17/30 [48:24<37:00, 170.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 4.059202059202059e-06, \"loss\": 0.6447899377346039, \"step\": 4450}\n",
      "{\"learning_rate\": 3.998069498069498e-06, \"loss\": 0.6794720339775085, \"step\": 4500}\n",
      "{\"learning_rate\": 3.9369369369369376e-06, \"loss\": 0.6682903409004212, \"step\": 4550}\n",
      "{\"learning_rate\": 3.875804375804375e-06, \"loss\": 0.6809291291236877, \"step\": 4600}\n",
      "{\"learning_rate\": 3.8146718146718146e-06, \"loss\": 0.6754169297218323, \"step\": 4650}\n",
      "{'acc': 0.5570599613152805}\n",
      "{\"eval_acc\": 0.5570599613152805, \"learning_rate\": 3.8e-06, \"loss\": 0.164522545337677, \"step\": 4662}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 18/30 [51:14<34:08, 170.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.7535392535392535e-06, \"loss\": 0.5250232005119324, \"step\": 4700}\n",
      "{\"learning_rate\": 3.6924066924066924e-06, \"loss\": 0.6749029588699341, \"step\": 4750}\n",
      "{\"learning_rate\": 3.6312741312741313e-06, \"loss\": 0.6791829884052276, \"step\": 4800}\n",
      "{\"learning_rate\": 3.5701415701415703e-06, \"loss\": 0.6765006363391877, \"step\": 4850}\n",
      "{\"learning_rate\": 3.509009009009009e-06, \"loss\": 0.682257376909256, \"step\": 4900}\n",
      "{'acc': 0.49709864603481624}\n",
      "{\"eval_acc\": 0.49709864603481624, \"learning_rate\": 3.483333333333333e-06, \"loss\": 0.2788227927684784, \"step\": 4921}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  63%|██████▎   | 19/30 [54:05<31:17, 170.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.4478764478764477e-06, \"loss\": 0.40016391038894655, \"step\": 4950}\n",
      "{\"learning_rate\": 3.386743886743887e-06, \"loss\": 0.6839032065868378, \"step\": 5000}\n",
      "{\"learning_rate\": 3.3256113256113256e-06, \"loss\": 0.6682731890678406, \"step\": 5050}\n",
      "{\"learning_rate\": 3.2644787644787645e-06, \"loss\": 0.6880892241001129, \"step\": 5100}\n",
      "{\"learning_rate\": 3.2033462033462034e-06, \"loss\": 0.6880411589145661, \"step\": 5150}\n",
      "{'acc': 0.5764023210831721}\n",
      "{\"eval_acc\": 0.5764023210831721, \"learning_rate\": 3.1666666666666667e-06, \"loss\": 0.4058400630950928, \"step\": 5180}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████▋   | 20/30 [56:56<28:27, 170.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 3.1422136422136424e-06, \"loss\": 0.27346638083457947, \"step\": 5200}\n",
      "{\"learning_rate\": 3.0810810810810813e-06, \"loss\": 0.6847812604904174, \"step\": 5250}\n",
      "{\"learning_rate\": 3.01994851994852e-06, \"loss\": 0.6849408197402954, \"step\": 5300}\n",
      "{\"learning_rate\": 2.9588159588159587e-06, \"loss\": 0.6748390209674835, \"step\": 5350}\n",
      "{\"learning_rate\": 2.8976833976833976e-06, \"loss\": 0.6620690727233887, \"step\": 5400}\n",
      "{'acc': 0.5783365570599613}\n",
      "{\"eval_acc\": 0.5783365570599613, \"learning_rate\": 2.85e-06, \"loss\": 0.5281515824794769, \"step\": 5439}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|███████   | 21/30 [59:46<25:36, 170.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.8365508365508366e-06, \"loss\": 0.15368939638137818, \"step\": 5450}\n",
      "{\"learning_rate\": 2.775418275418275e-06, \"loss\": 0.6789862954616547, \"step\": 5500}\n",
      "{\"learning_rate\": 2.7142857142857144e-06, \"loss\": 0.6817575550079346, \"step\": 5550}\n",
      "{\"learning_rate\": 2.653153153153153e-06, \"loss\": 0.6742636597156525, \"step\": 5600}\n",
      "{\"learning_rate\": 2.5920205920205923e-06, \"loss\": 0.6786787509918213, \"step\": 5650}\n",
      "{'acc': 0.5705996131528046}\n",
      "{\"eval_acc\": 0.5705996131528046, \"learning_rate\": 2.5333333333333334e-06, \"loss\": 0.6548512864112854, \"step\": 5698}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  73%|███████▎  | 22/30 [1:02:37<22:46, 170.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.530888030888031e-06, \"loss\": 0.02630922794342041, \"step\": 5700}\n",
      "{\"learning_rate\": 2.4697554697554697e-06, \"loss\": 0.6859728085994721, \"step\": 5750}\n",
      "{\"learning_rate\": 2.4086229086229087e-06, \"loss\": 0.6722018945217133, \"step\": 5800}\n",
      "{\"learning_rate\": 2.3474903474903476e-06, \"loss\": 0.6774450278282166, \"step\": 5850}\n",
      "{\"learning_rate\": 2.286357786357786e-06, \"loss\": 0.6750353813171387, \"step\": 5900}\n",
      "{\"learning_rate\": 2.2252252252252254e-06, \"loss\": 0.6817681312561035, \"step\": 5950}\n",
      "{'acc': 0.5705996131528046}\n",
      "{\"eval_acc\": 0.5705996131528046, \"learning_rate\": 2.216666666666667e-06, \"loss\": 0.10012189626693725, \"step\": 5957}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  77%|███████▋  | 23/30 [1:05:28<19:55, 170.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.164092664092664e-06, \"loss\": 0.5716609239578248, \"step\": 6000}\n",
      "{\"learning_rate\": 2.1029601029601033e-06, \"loss\": 0.6798175656795502, \"step\": 6050}\n",
      "{\"learning_rate\": 2.041827541827542e-06, \"loss\": 0.6744190764427185, \"step\": 6100}\n",
      "{\"learning_rate\": 1.9806949806949807e-06, \"loss\": 0.6676891541481018, \"step\": 6150}\n",
      "{\"learning_rate\": 1.9195624195624197e-06, \"loss\": 0.676351684331894, \"step\": 6200}\n",
      "{'acc': 0.5222437137330754}\n",
      "{\"eval_acc\": 0.5222437137330754, \"learning_rate\": 1.9e-06, \"loss\": 0.22459430932998659, \"step\": 6216}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 24/30 [1:08:19<17:04, 170.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.8584298584298584e-06, \"loss\": 0.4579344570636749, \"step\": 6250}\n",
      "{\"learning_rate\": 1.7972972972972973e-06, \"loss\": 0.6725033831596374, \"step\": 6300}\n",
      "{\"learning_rate\": 1.7361647361647362e-06, \"loss\": 0.6763861644268035, \"step\": 6350}\n",
      "{\"learning_rate\": 1.6750321750321752e-06, \"loss\": 0.6885830426216125, \"step\": 6400}\n",
      "{\"learning_rate\": 1.6138996138996139e-06, \"loss\": 0.6834577095508575, \"step\": 6450}\n",
      "{'acc': 0.5822050290135397}\n",
      "{\"eval_acc\": 0.5822050290135397, \"learning_rate\": 1.5833333333333333e-06, \"loss\": 0.3375275135040283, \"step\": 6475}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  83%|████████▎ | 25/30 [1:11:09<14:13, 170.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.5527670527670528e-06, \"loss\": 0.345102002620697, \"step\": 6500}\n",
      "{\"learning_rate\": 1.4916344916344917e-06, \"loss\": 0.6797918915748596, \"step\": 6550}\n",
      "{\"learning_rate\": 1.4305019305019307e-06, \"loss\": 0.6733975338935853, \"step\": 6600}\n",
      "{\"learning_rate\": 1.3693693693693694e-06, \"loss\": 0.6741164648532867, \"step\": 6650}\n",
      "{\"learning_rate\": 1.3082368082368083e-06, \"loss\": 0.6610724973678589, \"step\": 6700}\n",
      "{'acc': 0.5686653771760155}\n",
      "{\"eval_acc\": 0.5686653771760155, \"learning_rate\": 1.2666666666666667e-06, \"loss\": 0.4676744556427002, \"step\": 6734}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  87%|████████▋ | 26/30 [1:14:00<11:22, 170.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 1.2471042471042472e-06, \"loss\": 0.21238176941871642, \"step\": 6750}\n",
      "{\"learning_rate\": 1.1859716859716862e-06, \"loss\": 0.6662352550029754, \"step\": 6800}\n",
      "{\"learning_rate\": 1.1248391248391247e-06, \"loss\": 0.6781418526172638, \"step\": 6850}\n",
      "{\"learning_rate\": 1.0637065637065636e-06, \"loss\": 0.6771126234531403, \"step\": 6900}\n",
      "{\"learning_rate\": 1.0025740025740025e-06, \"loss\": 0.6700576853752136, \"step\": 6950}\n",
      "{'acc': 0.5841392649903289}\n",
      "{\"eval_acc\": 0.5841392649903289, \"learning_rate\": 9.5e-07, \"loss\": 0.5764009964466095, \"step\": 6993}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|█████████ | 27/30 [1:16:51<08:32, 170.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 9.414414414414415e-07, \"loss\": 0.09709309816360473, \"step\": 7000}\n",
      "{\"learning_rate\": 8.803088803088804e-07, \"loss\": 0.6697105026245117, \"step\": 7050}\n",
      "{\"learning_rate\": 8.191763191763191e-07, \"loss\": 0.6849208080768585, \"step\": 7100}\n",
      "{\"learning_rate\": 7.58043758043758e-07, \"loss\": 0.6765746521949768, \"step\": 7150}\n",
      "{\"learning_rate\": 6.969111969111969e-07, \"loss\": 0.6748162889480591, \"step\": 7200}\n",
      "{\"learning_rate\": 6.357786357786358e-07, \"loss\": 0.6719038033485413, \"step\": 7250}\n",
      "{'acc': 0.5802707930367504}\n",
      "{\"eval_acc\": 0.5802707930367504, \"learning_rate\": 6.333333333333333e-07, \"loss\": 0.023376752734184266, \"step\": 7252}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  93%|█████████▎| 28/30 [1:19:41<05:41, 170.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 5.746460746460746e-07, \"loss\": 0.6547416591644287, \"step\": 7300}\n",
      "{\"learning_rate\": 5.135135135135135e-07, \"loss\": 0.686141391992569, \"step\": 7350}\n",
      "{\"learning_rate\": 4.5238095238095237e-07, \"loss\": 0.6728278708457947, \"step\": 7400}\n",
      "{\"learning_rate\": 3.9124839124839125e-07, \"loss\": 0.6816840946674347, \"step\": 7450}\n",
      "{\"learning_rate\": 3.301158301158301e-07, \"loss\": 0.6770962882041931, \"step\": 7500}\n",
      "{'acc': 0.5783365570599613}\n",
      "{\"eval_acc\": 0.5783365570599613, \"learning_rate\": 3.1666666666666667e-07, \"loss\": 0.1504865002632141, \"step\": 7511}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  97%|█████████▋| 29/30 [1:22:32<02:50, 170.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.68983268983269e-07, \"loss\": 0.5232982003688812, \"step\": 7550}\n",
      "{\"learning_rate\": 2.0785070785070785e-07, \"loss\": 0.659570324420929, \"step\": 7600}\n",
      "{\"learning_rate\": 1.4671814671814672e-07, \"loss\": 0.6817832851409912, \"step\": 7650}\n",
      "{\"learning_rate\": 8.558558558558559e-08, \"loss\": 0.6819139671325684, \"step\": 7700}\n",
      "{\"learning_rate\": 2.4453024453024453e-08, \"loss\": 0.6735980486869813, \"step\": 7750}\n",
      "{'acc': 0.5802707930367504}\n",
      "{\"eval_acc\": 0.5802707930367504, \"learning_rate\": 0.0, \"loss\": 0.2721320569515228, \"step\": 7770}\n",
      "/wynton/protected/project/pathnlp/path_nlp_turing/output/frozen/pubmed_bert_0/SeminalVesicleNone/epoch-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [1:25:23<00:00, 170.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performance {'acc': 0.7021276595744681, 'checkpoint': 'epoch-1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fields = ['PrimaryGleason', 'SecondaryGleason', 'MarginStatusNone', 'SeminalVesicleNone']\n",
    "\n",
    "for field in fields:\n",
    "    print(field)\n",
    "    # Set output directories\n",
    "    args['output_dir'] = f\"{base_dir}/output/frozen/{args['model_type']}_{args['run']}/{field}\"\n",
    "    args['cache_dir'] = f\"{base_dir}/output/frozen/{args['model_type']}_{args['run']}/{field}\"\n",
    "    \n",
    "    kwargs = Namespace(**args)\n",
    "\n",
    "    # Create datasets\n",
    "    train_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['train']]\n",
    "    train_labels = [patient['labels'][field] for patient in data['train']]\n",
    "\n",
    "    val_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['val']]\n",
    "    val_labels = [patient['labels'][field] for patient in data['val']]\n",
    "\n",
    "    test_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['test']]\n",
    "    test_labels = [patient['labels'][field] for patient in data['test']]\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(train_labels)\n",
    "\n",
    "    # Handle new class\n",
    "    le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    le_dict = {str(key):le_dict[key] for key in le_dict}\n",
    "    \n",
    "    for label in val_labels + test_labels:\n",
    "        if str(label) not in le_dict:\n",
    "            le_dict[str(label)] = len(le_dict)\n",
    "\n",
    "    train_labels = [le_dict[str(label)] for label in train_labels]\n",
    "    val_labels = [le_dict[str(label)] for label in val_labels]\n",
    "    test_labels = [le_dict[str(label)] for label in test_labels]\n",
    "        \n",
    "    # Train model\n",
    "    model = BertForSequenceClassification.from_pretrained(bert_path, num_labels=len(le_dict))\n",
    "\n",
    "    documents_full = train_documents + val_documents + test_documents\n",
    "    patients_full = data['train'] + data['val'] + data['test']\n",
    "\n",
    "    p_test = len(test_labels)/len(patients_full)\n",
    "    p_val = len(val_labels)/(len(train_labels) + len(val_labels))\n",
    "\n",
    "    train_documents, test_documents, train_patients, test_patients = train_test_split(documents_full, \n",
    "                                                                                  patients_full, \n",
    "                                                                                  test_size= p_test,\n",
    "                                                                                  random_state=args['run'])\n",
    "\n",
    "    train_documents, val_documents, train_patients, val_patients = train_test_split(train_documents, \n",
    "                                                                                  train_patients, \n",
    "                                                                                  test_size= p_val,\n",
    "                                                                                  random_state=args['run'])\n",
    "\n",
    "\n",
    "    train_labels = [le_dict[str(patient['labels'][field])] for patient in train_patients]\n",
    "    val_labels = [le_dict[str(patient['labels'][field])] for patient in val_patients]\n",
    "    test_labels = [le_dict[str(patient['labels'][field])] for patient in test_patients]\n",
    "        \n",
    "    class_weights = make_weights_for_balanced_classes(train_labels, len(train_labels))\n",
    "    \n",
    "    train_dataset = path_dataset(train_documents, train_labels, model, tokenizer)\n",
    "    val_dataset = path_dataset(val_documents, val_labels, model, tokenizer)\n",
    "    test_dataset = path_dataset(test_documents, test_labels, model, tokenizer)\n",
    "    \n",
    "    train(kwargs, train_dataset, val_dataset, model, tokenizer, class_weights)\n",
    "\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff8943",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617ed260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_dataloader, args, model, tokenizer, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_task_names = (\n",
    "        (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n",
    "    )\n",
    "\n",
    "    results = {}\n",
    "   \n",
    "    # Eval!\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"labels\": batch[3],\n",
    "            }\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = (\n",
    "                    batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n",
    "                )  # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(\n",
    "                out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    result = compute_metrics('mnli', preds, out_label_ids)\n",
    "    results.update(result)\n",
    "    \n",
    "    results = {'macro': np.round(f1_score(out_label_ids, preds, average='macro'), 3),\n",
    "               'micro': np.round(f1_score(out_label_ids, preds, average='micro'), 3)}\n",
    "\n",
    "    return results, out_label_ids, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e872db93",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load config for '/wynton/protected/project/pathnlp/path_nlp_turing/output/fine_tuning/biobert_0/PrimaryGleason/epoch-19'. Make sure that:\n\n- '/wynton/protected/project/pathnlp/path_nlp_turing/output/fine_tuning/biobert_0/PrimaryGleason/epoch-19' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/wynton/protected/project/pathnlp/path_nlp_turing/output/fine_tuning/biobert_0/PrimaryGleason/epoch-19' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f2c9df75d6bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'tnlr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tnlr_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tnlrv3_classification'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[1;32m    579\u001b[0m                 \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \"\"\"\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             )\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for '/wynton/protected/project/pathnlp/path_nlp_turing/output/fine_tuning/biobert_0/PrimaryGleason/epoch-19'. Make sure that:\n\n- '/wynton/protected/project/pathnlp/path_nlp_turing/output/fine_tuning/biobert_0/PrimaryGleason/epoch-19' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/wynton/protected/project/pathnlp/path_nlp_turing/output/fine_tuning/biobert_0/PrimaryGleason/epoch-19' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "fields = ['PrimaryGleason', 'SecondaryGleason', 'MarginStatusNone', 'SeminalVesicleNone']\n",
    "results = {field: {'macro': [], 'micro': []} for field in fields}\n",
    "args['model_type'] = 'biobert'\n",
    "\n",
    "for i in range(1):\n",
    "    args['run'] = i\n",
    "    \n",
    "    for field in fields[:1]:\n",
    "\n",
    "        # Create datasets\n",
    "        train_labels = [patient['labels'][field] for patient in data['train']]\n",
    "        val_labels = [patient['clean_labels'][field] for patient in data['val']]\n",
    "        test_labels = [patient['clean_labels'][field] for patient in data['test']]\n",
    "\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(train_labels)\n",
    "\n",
    "        # Handle new class\n",
    "        le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "        le_dict = {str(key):le_dict[key] for key in le_dict}\n",
    "\n",
    "        for label in val_labels + test_labels:\n",
    "            if str(label) not in le_dict:\n",
    "                le_dict[str(label)] = len(le_dict)\n",
    "\n",
    "        documents_full = train_documents + val_documents + test_documents\n",
    "        labels_full = train_labels + val_labels + test_labels\n",
    "\n",
    "        p_test = len(test_labels)/len(labels_full)\n",
    "        p_val = len(val_labels)/(len(train_labels) + len(val_labels))\n",
    "\n",
    "        train_docs, test_docs, train_labels, test_labels = train_test_split(documents_full, \n",
    "                                                                                      labels_full, \n",
    "                                                                                      test_size= p_test,\n",
    "                                                                                      random_state=args['run'])\n",
    "\n",
    "        train_docs, val_docs, train_labels, val_labels = train_test_split(train_docs, \n",
    "                                                                                      train_labels, \n",
    "                                                                                      test_size= p_val,\n",
    "                                                                                      random_state=args['run'])\n",
    "\n",
    "        train_labels = [le_dict[str(label)] for label in train_labels]\n",
    "        val_labels = [le_dict[str(label)] for label in val_labels]\n",
    "        test_labels = [le_dict[str(label)] for label in test_labels]\n",
    " \n",
    "        best_performance = readJson(f\"{base_dir}/path_nlp_turing/output/fine_tuning/{args['model_type']}_0/{field}/best_performance.json\")\n",
    "        checkpoint_file = f\"{base_dir}/path_nlp_turing/output/fine_tuning/{args['model_type']}_0/{field}/{best_performance['checkpoint']}\"\n",
    "        config_file = f\"{base_dir}/path_nlp_turing/output/fine_tuning/{args['model_type']}_0/{field}/save_output/config.json\"\n",
    "            \n",
    "        if args['model_type'] != 'tnlr':\n",
    "            model = BertForSequenceClassification.from_pretrained(checkpoint_file, num_labels=len(le_dict))\n",
    "        else:\n",
    "            model = load_tnlr_base(checkpoint_file, config_file, model_type='tnlrv3_classification', num_labels=len(le_dict))\n",
    "        \n",
    "        model = model.cpu()\n",
    "\n",
    "        test_dataset = path_dataset(test_docs, test_labels, model, tokenizer)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=kwargs.per_gpu_train_batch_size)\n",
    "        output, labels, preds = evaluate(test_dataloader, kwargs, model, tokenizer, prefix=\"\")\n",
    "        results[field]['micro'].append(output['micro'])\n",
    "        results[field]['macro'].append(output['macro'])\n",
    "                                    \n",
    "        inv_le_dict = {v: k for k, v in le_dict.items()}\n",
    "\n",
    "        preds = [inv_le_dict[pred] for pred in preds]\n",
    "        labels = [inv_le_dict[label] for label in labels]\n",
    "                                    \n",
    "        y_actu = pd.Series(labels, name='Actual')\n",
    "        y_pred = pd.Series(preds, name='Predicted')\n",
    "        df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "                                    \n",
    "        print(f\"{field}_{args['run']}\")\n",
    "        print(df_confusion)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        del model\n",
    "        del test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5bb77de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrimaryGleason\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'PrimaryGleason'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-108990983881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'PrimaryGleason'"
     ]
    }
   ],
   "source": [
    "for field in fields:\n",
    "    print(field)\n",
    "    print(results[field][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387d47c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
